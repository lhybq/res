<a name="13daa31e"></a>
# 30 DCCF：解耦对比协同过滤
[TOC]

---

-  题目：DCCF：Disentangled Contrastive Collaborative Filtering
-  作者：University of Hong Kong **黄超团队** 
-  来源：SIGIR 23 
-  相关：协同过滤，对比学习，解耦表示 
-  评价：主要解决两个问题：
   - 一是意图解耦；
   - 二是自监督信号降噪；
-  论文：[https://arxiv.org/abs/2305.02759](https://arxiv.org/abs/2305.02759)
-  代码：[https://github.com/HKUDS/DCCF](https://github.com/HKUDS/DCCF) 
-  笔记总结：None 
-  模型框架：<br /> ![dde325cbfcb9757dcabf56f0bed2b505_3_Figure_1_-1981245633.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683687817858-6c55146f-0e8b-496d-9df1-f5b05b27d90f.png#averageHue=%23f8f6f3&clientId=u1ec26110-3040-4&from=ui&height=170&id=u0d19b3f6&originHeight=351&originWidth=1518&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=147270&status=done&style=none&taskId=u26013770-75dd-4e2f-959a-95bd250ab47&title=&width=734.1676025390625)

---

<a name="54467aa5"></a>
## 1.abstract：

- **背景：** 近年来，图神经网络(gnn)在协同过滤(CF)中的高阶关系建模中很受欢迎。在这个方向上，图对比学习(GCL)通过学习增强的用户和项目表征，在解决监督标签短缺问题上表现出了强大的性能。
- **动机：**其中不乏有很多优秀的模型，但仍有两个关键问题有待探讨:

i.现有的大多数基于协作的CF模型忽略了用户-物品交互行为通常是由多种潜在意图因素驱动的事实(例如，为家庭聚会购物，偏好的颜色或产品品牌);<br />ii.引入的非自适应增强技术容易受到噪声信息的影响，这引起了对模型鲁棒性的担忧，以及引入误导性自监督信号的风险。

- **方法：**鉴于这些局限性，我们提出了一个解耦对比协同过滤框架(DCCF)，**以自适应的方式实现带有自监督增强的意图解耦【创新】**。通过学习到的具有全局上下文的解耦表示，DCCF不仅能够从纠缠的自监督信号中提取出更细粒度的潜在因素，而且还可以减轻增强引起的噪声。最后，引入交叉视图对比学习任务，利用参数化交互掩码生成器实现自适应增强。

在各种公共数据集上的实验表明，与现有解决方案相比，我们的方法具有优越性。

---

<a name="Jkbsy"></a>
## 2.background & motivation

1. **背景**：GNN在协同过滤(CF)建模具有高阶连通性的用户-项目交互方面取得了显著的成功，如NGCF[32]、MCCF[35]、LightGCN[13]和GCCF[7]。基于GNN的CF模型通过迭代消息传递将用户-项目二部图结构编码为嵌入，用于协同信息聚合[37]。在潜在嵌入空间中捕获高阶用户(项目)相似度。
2. **研究现状：**
   1. 用户-物品交互，在现实世界的推荐系统中往往是高度稀疏的[41,49,50]；
   2. 最近的研究[39,42]试图将**对比学习**的力量与**GNN**结合起来，探索未标记的信息并提供自我监督信号。这些图对比学习(GCL)方法提出通过最大化对比增强视图之间的一致性来学习用户(项目)表示的不变性。通过遵循互信息最大化原则[22,28]，实现正对的一致性，并将负对的嵌入推开。
   3. 最近，在基于GCL的协同过滤中出现了两个关键的增强方案研究领域。具体而言：
      1. SGL[39]生成具有随机增强因子的对比视图，例如随机节点/边缘丢弃。
      2. 为了补充直接图连接，HCCF[42]和MHCN[46]提出要保证节点级表示和图级语义嵌入之间的一致性。
3. **动机：**我们认为当前的GCL推荐系统存在**两个关键的局限性**。
   1. 首先，由于偏好多样性，用户-项目交互背后的**潜在因素高度纠缠**，导致次优的增强的用户表示。用户项交互的形成是由许多意图因素驱动的[34，36]。<br />然而，在当前基于GCL的推荐方法中，学习的用户偏好与编码的不变表示纠缠在一起【不是很理解】，使得很难捕捉用户和项目之间更细粒度的交互模式。<br />这阻碍了捕捉真实用户偏好并提供准确的意图感知自我监督的能力。**因此，迫切需要一种新的方法来生成解耦的对比信号，用于信息增强。**
   2. 其次，许多现有的基于GCL的方法仍然**难以针对数据噪声提供准确的自监督学习（SSL）信号**，这使得对比学习难以适应具有不同结构的用户-项目交互图。<br />具体而言，引入的随机增强策略[39]可能无法很好地保留原始语义关系。<br />此外，尽管一些方法通过对所有节点的自判别将图级语义纳入辅助自监督信号[42，46]，但它们的模型性能容易受到用户交互数据噪声的影响，如误点击或流行度偏差。<br />在对比增强框架下，如果不区分节点或边的SSL信号重要性，就通过用来自噪声节点的自监督信号来补充主要推荐任务【具有许多误点击行为或高度符合流行偏好的用户】，就很容易产生偏误[30]。
4. **方法**：提出一种新的**基于对比学习的协作过滤模型，称为DCCF**。<br />考虑用户和项目之间的全局依赖性来编码多意图表示。通过设计**局部级节点**和**全局级意图原型**之间的意图感知信息**传递**和**聚合**来实现这一点。目标是识别重要的图结构信息，这些信息可以捕获准确且有用的环境不变模式，并具有意图解耦。通过这种方式，可以防止带有严重噪声信号的自监督信号提取。<br />为了实现这目标，创建了参数化的**边掩码生成器**，用于捕获用户和项目之间的隐式关系，并注入了意图感知的全局依赖关系。<br />因此，**图结构掩蔽器**可以自然地捕捉每个交互的重要性【对用户-项目关系的自适应】，用于对比增强。
5. **成果**：
   1. **将图对比学习应用于协同过滤带有自监督噪声的意图解耦**。
   2. 开发了一种称为DCCF的模型，该模型具有**参数化掩码生成器**，可自适应地构建全局上下文增强的**解耦GNN架构**。增强了推荐器的鲁棒性和泛化能力。

---

<a name="ydtqw"></a>
## 3.related work
<a name="bGhdt"></a>
### GNNs-based Recommender Systems. 
> 基于GNN的推荐器通过图结构执行递归消息传递，以对高阶协作关系进行建模[8，40，44]。例如，图卷积网络已被广泛用作编码器，以对用户-项目交互图进行建模，如LightGCN、LR-GCCF[7]和HGCF[25]。此外，图增强的注意力机制明确区分了对相邻节点之间嵌入传播的影响，并在各种推荐器中充当重要组件，包括社交关系学习DGRec[24]、多行为推荐[43]、基于知识图的推荐器KGAT[31]、JNSKR[4]。

<a name="eNJUF"></a>
### Recommendation with Disentangled Representations.
> 从隐式反馈中学习用户潜在意图的解耦表示一直是一个热门话题。已经提出了各种方法，例如使用变分自动编码器对高级用户意图进行编码，以改进推荐[20]。**DGCF[34]建立在意图解耦的思想之上**，并在具有嵌入分裂的图神经网络上执行解耦的表示学习。为了将来自用户或项目领域的辅助信息合并到推荐中，DisenHAN[36]试图利用异构图注意力网络来学习解耦的用户/项目表示。KGIN[33]是一种旨在使用项目知识图对潜在用户意图进行编码以提高推荐性能的方法。DCF[10]将用户和项目分解为因素级表示，并使用因素级注意力机制来捕捉潜在意图。在CDR[6]中，设计了一种动态路由机制来表征用户意图之间的相关性，以便嵌入去噪。然而，**大多数现有的解耦推荐系统都是以完全监督的方式构建的，这可能会受到现实世界场景中用户-项目交互的稀疏性的限制。**我们提出了一种新的模型利用自我监督学习来意图感知增强。

<a name="nWag5"></a>
### Contrastive Learning in Recommendation.
> 对比学习（CL）在各种推荐场景中获得了相当大的关注，如顺序推荐[9]、知识图增强推荐[51]、多兴趣推荐[47]和多行为推荐[38]。推荐系统中最相关的研究方向是通过对比学习来增强基于图神经网络的协同过滤。为此，最近提出的几个模型，如SGL[39]、NCL[17]和HCCF[42]，通过**利用对比增强**实现了最先进的性能。例如，SGL[39]使用随机丢弃破坏交互图结构以进行增强。在NCL[17]中，表示对齐是在单个用户和以语义为中心的节点之间执行的。虽然这些模型在提高推荐准确性方面是有效的，但它们可能**无法对用户-项目交互背后的潜在因素进行编码，使用粗粒度的用户偏好建模进行推荐可能会导致次优表示**。


---

<a name="VL4MF"></a>
## 4.methodology
<a name="jltxO"></a>
### 4.1意图表示解耦
<a name="LmBJv"></a>
#### 潜在意图因素建模：
符号表示：**![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683860485007-51ad5fa8-a556-473b-bbb7-016526e75705.png#averageHue=%23f0ece8&clientId=u70bcf1f8-60e1-4&from=paste&height=108&id=u524501ac&originHeight=119&originWidth=687&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=30353&status=done&style=none&taskId=u8b3a980c-6a7f-4e73-b639-7b3991d81dd&title=&width=624.5454410087966)<br />意在估计某个用户在观察交互的下，选择某个物品的可能性。即估计$P(y|u_i,v_j)$,其中$u_i,v_j$是用户和物品，$y$是学习的偏好分数。<br />	在与物品互动时，用户通常有不同的意图，_例如对特定品牌的偏好或对电影类型和演员的兴趣[21，48]_。为了捕捉这些不同的意图，我们假设𝐾 个不同的意图$c_u$ 和$c_v$分别来自用户端和物品端。物品方面的意图也可以理解为物品的上下文，_例如打算在情人节购物的用户可能更喜欢具有“浪漫”上下文的物品。_对用户项目偏好的预测目标可以如下所示：$1. \int_{c_u}\int_{c_v} P(y,c_u,c_v|u,v)\,\mathrm{d}{c_v}\mathrm{d}{c_u}$。用户-项目交互概率𝑦 由潜在意图$c_u$ 和$c_v$决定， 并且可以使用以下公式导出：3. $\sum_{k}^{K} P(y,c_u^k,c_v^k|u,v)=\sum_{k}^{K} P(y|c_u^k,c_v^k)P(c_u^k|u)P(c_v^k|v)=\mathrm{E_{P(c_u|u)P(c_v|v)}} \left[P(y|c_u,c_v)\right]$.<br />$f(.)$表示意图编码的预测函数，根据[29,30]上式近似为：4. 1$\mathrm{E_{P(c_u|u)P(c_v|v)}} \left[f(c_u,c_v)\right]\approx f(\mathrm{E_{P(c_u|u)}} \left[c_u\right],\mathrm{E_{P(c_v|v)}} \left[c_v\right])$在上述推断中，近似误差被称为Jensen gap[1]，在我们的预测函数$f(.)$中可以很好地定界。
<a name="g7M7p"></a>
#### 具有全局上下文的多意图表示：
在现有的推荐系统中，**意图多样性已经通过解耦的表示进行了编码，但全局层面的意图感知协作关系在很大程度上被忽视了**。全局级用户（物品）依赖性建模可以增强基于GNN的消息传递模型对**稀疏性和过度平滑问题**的鲁棒性[42]，通过以不受局部连接限制地传播信息。因此，我们建议通过局部和全局级的信息传播嵌入来理清用户和物品之间的协作关系。<br />**图消息传递：**通过使用用户表示的基于图的消息传递框架来构建DCCF模型。通常，我们的消息传播层用用户/物品嵌入矩阵$E^{(u)} \in R^{I*d}, E^{(v)} \in R^{J*d}$形式化表示的:  5. $Z^{(u)}=\overline{A}\cdot E^{(u)}, Z^{(v)}=\overline{A}^T\cdot E^{(v)}$,其中标准化的转移矩阵为$\overline{A}=D^{1/2}_{(u)}\cdot A\cdot D^{1/2}_{(u)}$,为了利用高阶协同滤波信号，我们在不同的图层之间执行基于GNN的嵌入传播，例如从(𝑙−1)层至(𝑙)层 6. ![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683900919682-dddf5fa7-d4ea-42a6-9375-108de21c2dbe.png#averageHue=%23f4f2ef&clientId=u70bcf1f8-60e1-4&from=paste&height=30&id=uea8e33b4&originHeight=50&originWidth=430&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=4920&status=done&style=none&taskId=u3d307fff-a44f-4e62-8abf-96c17ab868a&title=&width=254.90054321289062)，为了抑制过平滑效应，将残差连接应用于聚合阶段[7，42]。<br />**意图感知信息聚合：**在多意图编码器中，解耦的用户-物品偏好保存在$\mathrm{E_{P(c_u|u)}} \left[c_u\right],\mathrm{E_{P(c_v|v)}} \left[c_v\right]$中，在DCCF中，为用户和物品分别定义K个全局意图${\{ c^k_u \in R^d \}}_{k=1}^K and {\{ c^k_v \in R^d \}}_{k=1}^K$,通过这些可学习的意图嵌入，我们通过聚合全局范围内不同的K个意图原型生成用户和物品嵌入，例如在第L层，使用以下设计：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683901954578-3a134f52-0d85-47b8-9314-76af1f792a4b.png#averageHue=%23f8f7f5&clientId=u70bcf1f8-60e1-4&from=paste&height=121&id=u7d3344e8&originHeight=172&originWidth=460&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=17259&status=done&style=none&taskId=u3e19acab-cb1d-433d-9e59-cacb579f852&title=&width=324.1647644042969)<br />$e^{(u)},e^{(v)}$分别表示用户物品嵌入，用户$u_i$和每个意图原型$c_u$的相关性得分为$P(c^k_u|e^{(u)}_{i,j})$:<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683902701776-47154a7e-cd64-432e-b768-2438390b9014.png#averageHue=%23f7f6f4&clientId=u70bcf1f8-60e1-4&from=paste&height=102&id=uf4870cd3&originHeight=126&originWidth=649&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=16178&status=done&style=none&taskId=ue21b3185-2284-46d0-8306-077cada5090&title=&width=523)<br />𝜂(.)是exp(.)，在生成传播的消息后，通过将**局部协同过滤信号**与**全局解耦的协同关系**相结合来对其再定义，如下所示：![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683902999031-40b13e19-a812-40e4-a79d-7b13e5c87259.png#averageHue=%23f4f1ee&clientId=u70bcf1f8-60e1-4&from=paste&height=33&id=u5c7baf41&originHeight=47&originWidth=648&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=6791&status=done&style=none&taskId=u61bf30e1-5ecc-4ad7-b5f2-c65f7b68749&title=&width=457.9886474609375)<br />$R_l^{(u)},R_l^{(v)}$是$r_{i,l}^{(u)},r_{j,l}^{(v)}$的矩阵表达。基于此，将意图解耦结合到图神经架构中，使我们的学习表示能够有效地解开驱动复杂用户-项目交互行为的潜在因素。

<a name="ZxMBF"></a>
### 4.2 解耦对比学习
本文探索了**使用意图解耦的对比增强来解决推荐系统中的数据稀疏问题的潜力**。虽然可以通过最大化对比视图之间正对之间的一致性来产生自监督信号，但本文认为这种增强容易受到数据噪声的影响，_例如误点击。_有噪声的对比规则化可能会误导自监督的学习过程。_例如，通过节点在噪声交互边上的自区分来增强模型以实现嵌入一致性，可能涉及噪声自监督信号并导致次优表示。_<br />为了解决这一问题，本文设计可学习的增强器，既考虑了**局部协作关系**，又考虑了**全局解耦的用户(项)依赖关系**。通过这样做，可学习的对比增强可以自适应地学习解耦的自监督信号。
<a name="sxr0S"></a>
#### 解耦数据增强
为了使增强能够自适应每个连接跳，为每一层GNN引入了可学习的关系矩阵![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683944164954-4444a8a6-59dc-4d5d-9b49-c8f30cfaeccc.png#averageHue=%23f0ece7&clientId=ue50be3c5-19f3-4&from=paste&height=27&id=ua73c35c5&originHeight=30&originWidth=108&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=2052&status=done&style=none&taskId=u4900e315-d12d-4956-b2ed-aa0457eda13&title=&width=98.18181605378463)，用于编码用户和物品之间的隐式关系。受先前图去噪工作[18,26]的启发，我们的目标是生成图掩盖![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683944309481-da680e81-cf73-4644-a51d-cd7d907bd66b.png#averageHue=%23f1ede8&clientId=ue50be3c5-19f3-4&from=paste&height=27&id=u73050f8e&originHeight=30&originWidth=121&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=2132&status=done&style=none&taskId=uc9c22577-23fe-426e-bb55-14cce945cd9&title=&width=109.99999761581427), 其可用于通过逐元素乘法获得关系矩阵![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683944345847-0bdcc0cd-46ef-43ba-813f-7e0e8fe3cf20.png#averageHue=%23ede9e5&clientId=ue50be3c5-19f3-4&from=paste&height=25&id=u26c0b95e&originHeight=28&originWidth=153&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=2354&status=done&style=none&taskId=u0a89b831-78ac-4838-ae55-fd7d2cfaff8&title=&width=139.0909060761949)<br />**图掩盖学习：<br />	**对于![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683945320867-a8519b28-95df-49c4-ae04-5db626cee520.png#averageHue=%23ede9e5&clientId=ue50be3c5-19f3-4&from=paste&height=7&id=ud413c248&originHeight=28&originWidth=153&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=2354&status=done&style=none&taskId=u2aca02cd-2939-400f-b2a5-438f1e4163f&title=&width=38)的每个元素![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683945366060-705432f5-6dcb-4c3f-80b1-83cfeb11236c.png#averageHue=%23f2efeb&clientId=ue50be3c5-19f3-4&from=paste&height=25&id=u7a4c271c&originHeight=39&originWidth=141&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=2269&status=done&style=none&taskId=ue96ea243-acf7-4e0b-8bcd-a6fc01b64ec&title=&width=89.16667175292969)，反映用户i和物品j之间的交互的掩盖程度，其值越接近0，交互越不重要。在DCCF中，基于解耦嵌入![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683945754248-d90fed47-473a-4a88-adc7-d4a49e170fab.png#averageHue=%23f5f0ec&clientId=ufd74ca67-5014-4&from=paste&height=24&id=u276d4af9&originHeight=38&originWidth=84&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1835&status=done&style=none&taskId=u8bb09b3e-09f6-4953-a5a0-82ecaded172&title=&width=53.76)和![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683945767950-38b54207-e529-4204-a8d8-05cc60e3a395.png#averageHue=%23eee9e2&clientId=ufd74ca67-5014-4&from=paste&height=13&id=u01e33ad9&originHeight=21&originWidth=47&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=663&status=done&style=none&taskId=u07f8f7ab-1452-4785-929b-cb1c9ee2366&title=&width=28.079999923706055)![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683945778280-67ba1745-779a-4bf7-a7a6-ca7471298cc1.png#averageHue=%23f5efe9&clientId=ufd74ca67-5014-4&from=paste&height=22&id=uc8f6c1fb&originHeight=34&originWidth=49&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1254&status=done&style=none&taskId=u9a87d5b4-8429-4150-955b-44dddc9cff6&title=&width=31.36)获得![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683945815461-dee869bc-6a9d-4af4-8d19-a97202b4971d.png#averageHue=%23f1ece7&clientId=ufd74ca67-5014-4&from=paste&height=21&id=u45c56333&originHeight=33&originWidth=40&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1000&status=done&style=none&taskId=u2a0086f2-611e-4002-bb67-58c66a17dc2&title=&width=25.6)进而保留意图感知交互模式。具体而言，我们使用节点嵌入之间的余弦相似性[11，26]来衡量交互的重要性：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683945964712-a5e1efe3-6367-4efb-bf74-7d13c1fe8295.png#averageHue=%23f7f5f4&clientId=ufd74ca67-5014-4&from=paste&height=69&id=u57c15a29&originHeight=129&originWidth=374&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=9770&status=done&style=none&taskId=u1f8c0a38-54c0-492c-8b91-ffe22717bd5&title=&width=199.46666666666667)<br />掩盖值是通过使用公式将相似度的范围线性变换为[0，1]而获得的：![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683946067698-81dca7bb-dbdd-45b3-b74e-0d7e44975901.png#averageHue=%23f7f3ef&clientId=ufd74ca67-5014-4&from=paste&height=29&id=ua84e27f0&originHeight=36&originWidth=234&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=3990&status=done&style=none&taskId=ua039d997-5da1-46fc-9c01-b37c04c3cb0&title=&width=185.8000030517578)<br />**可学习的增强：**<br />当用户i和物品j之间没有交互的时候，![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683946177817-8395058e-c404-4780-b0c2-6991975480ad.png#averageHue=%23f3efe9&clientId=ufd74ca67-5014-4&from=paste&height=18&id=u0c96c8e9&originHeight=26&originWidth=40&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=805&status=done&style=none&taskId=ud2299996-4728-4c5b-a862-d765e7f9fc9&title=&width=27.33333396911621)为0。![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683946224740-7340a81e-513d-44d0-aa98-859c8a9be74c.png#averageHue=%23eee9e3&clientId=ufd74ca67-5014-4&from=paste&height=20&id=u3e88c37a&originHeight=27&originWidth=26&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=680&status=done&style=none&taskId=u4eeea63c-008e-4ddc-87b8-9da8f715bc7&title=&width=18.86666774749756)通过![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683946269833-3f5e864b-e538-4453-a02c-dc09f2b11491.png#averageHue=%23f0eae4&clientId=ufd74ca67-5014-4&from=paste&height=17&id=ufb3efef0&originHeight=25&originWidth=94&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1645&status=done&style=none&taskId=ueb7cf686-90d2-45a4-9b6b-e2581bbda42&title=&width=65.13333511352539)元素相乘得到。为了计算简单性，仅计算观察到的相互作用的掩码值。利用学习的关系矩阵，我们用节点的阶数将其归一化如下：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683946788094-5d61de4d-9e77-42ab-bd4c-fdd5e7af41a6.png#averageHue=%23f9f7f6&clientId=ufd74ca67-5014-4&from=paste&height=53&id=UtLK9&originHeight=73&originWidth=374&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=5024&status=done&style=none&taskId=u62b154a7-9302-4430-a880-e264a176278&title=&width=270.4666748046875)<br />为了将自适应增强与消息传递集成，我们应用归一化学习关系矩阵![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683946901944-6c39f7dd-8ed4-41f5-89b2-6794fc8cf59e.png#averageHue=%23ede7e1&clientId=ufd74ca67-5014-4&from=paste&height=22&id=ufd1359fe&originHeight=25&originWidth=28&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=728&status=done&style=none&taskId=uc4721c8f-bc84-400f-9b2b-e6a22e41fa8&title=&width=24.933334350585938)通过节点的消息进行可学习的传播。通过这种设计，我们扰动图结构，以生成具有自适应增强的对比学习视图。自适应掩蔽的增强可以形式化地表示如下：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683946935164-57ee28ae-1b58-449c-9215-d88706cba548.png#averageHue=%23f7f4f2&clientId=ufd74ca67-5014-4&from=paste&height=34&id=ub14bbce8&originHeight=42&originWidth=315&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=3480&status=done&style=none&taskId=u95e65e9a-2463-48a6-8591-6e63c352a09&title=&width=256.00001525878906)<br />为了生成多个对比视图，我们考虑了局部协作信号和全局解耦关系。特别地，我们在编码的局部嵌入![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683947136965-604abbe1-737c-41f8-9086-7cd53b62a3e9.png#averageHue=%23f3f0eb&clientId=ufd74ca67-5014-4&from=paste&height=25&id=u9b94352a&originHeight=36&originWidth=195&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=3370&status=done&style=none&taskId=u2e5375bc-5e64-423a-a6be-83d981bbed4&title=&width=134.00000762939453)，以及具有意图解耦的全局嵌入![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683947155456-045883e1-1fc0-4d37-8860-16d511b9b089.png#averageHue=%23f4f0ec&clientId=ufd74ca67-5014-4&from=paste&height=26&id=u010113ea&originHeight=37&originWidth=204&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=3381&status=done&style=none&taskId=uf8dd21d6-9c75-420f-99b5-10e6b5c764d&title=&width=141.8000030517578)使用两个可学习的掩码矩阵来执行扩充。我们获得两个掩码矩阵![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683947228865-6ec81181-1069-46f4-bba6-0162d7ec8ce3.png#averageHue=%23f3eee9&clientId=ufd74ca67-5014-4&from=paste&height=27&id=u451c001c&originHeight=34&originWidth=42&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1014&status=done&style=none&taskId=uedf775d5-a88b-44e2-94c5-1eb38855035&title=&width=33.400001525878906)分别使用以下公式：![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683947254561-9660cf6d-c545-481c-8aa1-fd7725389b67.png#averageHue=%23f6f3ee&clientId=ufd74ca67-5014-4&from=paste&height=30&id=u4960727d&originHeight=37&originWidth=498&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=7017&status=done&style=none&taskId=u5c739b2b-03ce-42ad-9a81-ecba7db2a5d&title=&width=406.6000061035156)<br />之后，我们的增强感知消息传递范式可以用以下嵌入细化细节来描述：![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683947295027-353b98a0-25bf-4e6d-bd13-6d7bdb308b6d.png#averageHue=%23f6f3f0&clientId=ufd74ca67-5014-4&from=paste&height=34&id=u6d344324&originHeight=45&originWidth=373&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=4555&status=done&style=none&taskId=u64ffe5e3-a87d-4ab4-aba7-a560b919b3f&title=&width=278.933349609375)其中![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683947337097-a9b74720-22c5-41d9-8a17-da3d9c6d129c.png#averageHue=%23f3efeb&clientId=ufd74ca67-5014-4&from=paste&height=23&id=u66dfd453&originHeight=38&originWidth=159&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=2700&status=done&style=none&taskId=ua2453e7f-e36a-4779-bac6-9e19e9fc7d3&title=&width=96.80000305175781)分别表示局部和全局级别的增强表示。类似地，项目嵌入以类似的方式进行融合。
<a name="ziBMa"></a>
#### 对比学习
使用上面的增强表示视图，我们在用户和物品的不同视图特定嵌入之间进行对比学习。遵循[39,42]中的监督对比信号方法，我们使用来自**原始CF视图和每个增强视图的同一用户（项目）的嵌入来生成每个正对**。不同节点的编码表示被视为负对。具体而言，我们使用我们的增强器生成**三个增强视图**：i）具有自适应增广的局部协作视图![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683947936278-cfcc7c53-4d07-4ca6-96de-d5be61a8a0fb.png#averageHue=%23efe8e2&clientId=ufd74ca67-5014-4&from=paste&height=23&id=ua894b47a&originHeight=27&originWidth=70&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1523&status=done&style=none&taskId=u65d2719b-e265-41d0-9964-e83bd1927b2&title=&width=59.333335876464844); ii）解耦的全局协作视图![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683947991644-a29523f8-048a-426f-978d-8d721fa5e49a.png#averageHue=%23efe9e2&clientId=ufd74ca67-5014-4&from=paste&height=24&id=u57af0447&originHeight=27&originWidth=53&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1183&status=done&style=none&taskId=u8a8bd127-86bf-409e-8e7d-19a7bc0fb4f&title=&width=46.26666831970215); 以及iii）自适应增强视图![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683948012667-60fae061-1f89-4f80-848e-784f36e40d09.png#averageHue=%23f1ebe6&clientId=ufd74ca67-5014-4&from=paste&height=24&id=u7ab1a1f6&originHeight=29&originWidth=71&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1382&status=done&style=none&taskId=u39b716a4-9aa6-4fe9-8a3c-c06eb1e334d&title=&width=57.866668701171875). 我们使用InfoNCE损失生成对比自监督信号，如下所示：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683948067230-6d2eed7b-51fb-4ec6-923b-512b9a561f0f.png#averageHue=%23f6f4f1&clientId=ufd74ca67-5014-4&from=paste&height=82&id=u067d8868&originHeight=83&originWidth=456&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=10829&status=done&style=none&taskId=uf4936e2f-a980-4ec8-87db-90b171139b8&title=&width=448.20001220703125)<br />m代表原始视图的节点嵌入视图，其使用GNN编码基本嵌入$z \in Z^{(u)}$得到；n表示从三个增强嵌入中采样![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683948499764-5f19b6aa-9528-4a0d-9d8d-5b2c41cda6a1.png#averageHue=%23f2ede9&clientId=ufd74ca67-5014-4&from=paste&height=23&id=u439d5c0e&originHeight=29&originWidth=299&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=3901&status=done&style=none&taskId=uc64b4f57-1732-4a96-b4ee-b9a5ce811db&title=&width=239.4666748046875)，余弦相似度函数表示为s(·)。用户侧的对比学习损失可以形式化为:<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683948590872-677f4475-0639-4014-8aba-e609b3cd8756.png#averageHue=%23f6f4f1&clientId=ufd74ca67-5014-4&from=paste&height=32&id=u9e544795&originHeight=39&originWidth=311&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=4039&status=done&style=none&taskId=ub2703eb2-e566-420c-950a-ba8fd7e8c49&title=&width=253.86666870117188)<br />通过堆叠层图神经网络，特定层的嵌入层聚合不同层的形式如下:![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683948687525-5b877a83-ba0c-4d38-93d0-1595b1eb9bb9.png#averageHue=%23f5f1ee&clientId=ufd74ca67-5014-4&from=paste&height=29&id=u5e74cb4a&originHeight=38&originWidth=146&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=2459&status=done&style=none&taskId=ucd91cdbd-d416-47f1-b8a4-255c8dde242&title=&width=112.86666870117188)![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683948709549-95cde2d3-f47e-4406-b92a-b14b40bc559b.png#averageHue=%23f4f0ed&clientId=ufd74ca67-5014-4&from=paste&height=29&id=ufd1fe7b7&originHeight=36&originWidth=139&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=2332&status=done&style=none&taskId=ubca848fc-8779-484b-931f-6f7403ba357&title=&width=112.13333892822266)用户-项目偏好得分推导为:<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683948734363-e7d28e30-79c4-4e8c-8480-1e020a12de54.png#averageHue=%23f8f5f3&clientId=ufd74ca67-5014-4&from=paste&height=37&id=u50257493&originHeight=41&originWidth=331&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=4169&status=done&style=none&taskId=u89a055e8-1d97-4aa6-951d-604ccdc00ab&title=&width=302.53334045410156)<br />为了使用估计的偏好分数来优化经典的监督推荐任务，使用了以下贝叶斯个性化排名(BPR)损失:<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683948773808-cbd06d12-42a1-4cc0-8812-919a1ecd131b.png#averageHue=%23f9f7f5&clientId=ufd74ca67-5014-4&from=paste&height=56&id=u0313e577&originHeight=64&originWidth=356&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=5630&status=done&style=none&taskId=u03fd0479-db2e-46fe-a5ec-9826f0d89b3&title=&width=309.86668395996094)<br />其中R为每个mini-batch中采样相互作用的集合[13]。对于每个用户$u_i$ 我们从训练数据中抽样S个正项目![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683952568901-10bdfb23-7af2-4636-9b7d-a111d4ede265.png#averageHue=%23efe9e3&clientId=ufd74ca67-5014-4&from=paste&height=18&id=u75fea301&originHeight=26&originWidth=131&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=2106&status=done&style=none&taskId=u7c5ac31b-144a-41f9-88b3-d553c267dcc&title=&width=92.86666870117188)S个负项目![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683952586485-51669b7b-a789-41f5-a19f-0f97d79da2b1.png#averageHue=%23eee9e3&clientId=ufd74ca67-5014-4&from=paste&height=21&id=udcb8ee42&originHeight=25&originWidth=132&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=2199&status=done&style=none&taskId=ue43484d4-45ef-4463-a9e4-23bf883b090&title=&width=113.4000015258789)。<br />最后，我们将自监督损失和经典推荐损失整合到一个多任务学习目标中，如下所示:<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683952614501-ddbdb7d5-9f4c-444d-b2e6-aaeb2c7d0273.png#averageHue=%23f6f2ee&clientId=ufd74ca67-5014-4&from=paste&height=27&id=u3f72b1fd&originHeight=34&originWidth=481&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=6333&status=done&style=none&taskId=u2b52927b-73ed-441c-8a1e-db261dd3c8f&title=&width=384.5333557128906)<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1683952646218-31e5b237-5226-4163-8880-1e5a6787c3ef.png#averageHue=%23f4f0ed&clientId=ufd74ca67-5014-4&from=paste&height=50&id=u334aa36f&originHeight=68&originWidth=551&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=14317&status=done&style=none&taskId=uf2f19eef-57bc-40bd-b6f8-c396685590f&title=&width=408.8666687011719)
<a name="X63dh"></a>
### 4.3模型讨论
针对特定用户$u_i$， 将相应的对比自监督学习信号与![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1684027986084-d382daeb-5d74-4eaf-b77b-57b07cddc2a3.png#averageHue=%23f6f1ec&clientId=ufd74ca67-5014-4&from=paste&height=22&id=ude0b4c61&originHeight=32&originWidth=108&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1996&status=done&style=none&taskId=ud4406ff0-f4b0-46cc-a600-d1f96f98827&title=&width=72.60000228881836), 其中![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1684027996049-7eb8f52f-c8c2-47aa-8d8d-0b70b862503e.png#averageHue=%23f6f1ec&clientId=ufd74ca67-5014-4&from=paste&height=9&id=u0cf038a6&originHeight=32&originWidth=108&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1996&status=done&style=none&taskId=u4c94c6db-9705-4306-a2b3-a5f62b41eb3&title=&width=30)是的编码嵌入$u_i$从具有意图感知的用户全局依赖性的增强。![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1684031460161-444361ad-4b59-483a-a2a1-d1c78e7273c2.png#averageHue=%23f6f2ed&clientId=ufd74ca67-5014-4&from=paste&height=22&id=ud0cf7521&originHeight=35&originWidth=106&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=1995&status=done&style=none&taskId=u1e8c7388-052d-4805-93ac-42730afa86f&title=&width=66.5333366394043)的梯度关于解耦表示$r^{(𝑢)}_𝑖$ 由负样本贡献的结果可以推导如下：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1684031507038-741434b4-92a5-47d9-b5f7-455492f7646c.png#averageHue=%23f9f7f5&clientId=ufd74ca67-5014-4&from=paste&height=122&id=u0bc148e4&originHeight=154&originWidth=385&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=14563&status=done&style=none&taskId=u1d14b8e6-d412-45f8-9735-5f5e3dafa00&title=&width=304.3333435058594)<br />我们省略了图层的索引。![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1684031904257-4fa71340-7431-4e12-8f94-0d2b4bc7c2ef.png#averageHue=%23f4f0ec&clientId=ufd74ca67-5014-4&from=paste&height=21&id=u163135f2&originHeight=30&originWidth=492&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=6460&status=done&style=none&taskId=u6ca0420b-894b-470d-a780-a24266c5563&title=&width=342.4000244140625)二范数与以下函数成比例：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1684031962038-0bc9a4b0-db85-4dd5-84e6-48e9abf77dd5.png#averageHue=%23f8f6f4&clientId=ufd74ca67-5014-4&from=paste&height=44&id=u2b6f5e03&originHeight=64&originWidth=423&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=6982&status=done&style=none&taskId=u55a30f53-3ea0-432c-8ad9-8f97f7a6829&title=&width=287.6000061035156),其中![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1684031992987-994946af-c2e6-402c-a208-37e2caf8d543.png#averageHue=%23f7f4f0&clientId=ufd74ca67-5014-4&from=paste&height=26&id=ucba608c1&originHeight=36&originWidth=183&originalType=binary&ratio=1.875&rotation=0&showTitle=false&size=2759&status=done&style=none&taskId=ub1bd3cce-9e81-4a42-b478-fd1d99f570e&title=&width=134.60000610351562)【该部分略】

<a name="HjzCW"></a>
## 5.实验复现
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1684070424124-858a6b02-eb3b-43b8-a49a-122a567887a4.png#averageHue=%23ada7a5&clientId=uf9f964bd-b608-4&from=paste&height=380&id=u64984948&originHeight=418&originWidth=1121&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=159550&status=done&style=none&taskId=ua5727920-3274-44eb-8320-744f8cda1bf&title=&width=1019.090887002709)<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1684070437236-0c017b9f-a5ed-41c1-a1e8-3ed58db2951e.png#averageHue=%23f5f2ef&clientId=uf9f964bd-b608-4&from=paste&height=453&id=u9d62ce1d&originHeight=498&originWidth=1268&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=360538&status=done&style=none&taskId=u852d1e1c-8d9f-46e8-bc9f-18f84ffc8b9&title=&width=1152.7272477425824)
<a name="ZZjvf"></a>
## 6.总结
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1684070465081-af2207c7-e5bf-4317-8f4c-eccbe848b075.png#averageHue=%23f5f4f3&clientId=uf9f964bd-b608-4&from=paste&height=377&id=u28b22684&originHeight=500&originWidth=1042&originalType=binary&ratio=1.100000023841858&rotation=0&showTitle=false&size=93208&status=done&style=none&taskId=ubb9a8799-b5fc-463e-8aa4-2e9c0a68391&title=&width=786.272705078125)<br />本文提出了一种解耦的对比学习方法，该方法探索了潜在的交互隐含意图。<br />我们引入了一个图结构学习层，该层能够基于学习到的解耦的用户(物品)意图感知来实现自适应交互增强。<br />沿着增强的意图感知图结构，我们提出了一种意图感知对比学习方案。

> 对于未来的工作，一个潜在的方向是将解耦的**表示学习与因果分析相结合**，以解决噪声交互数据的偏差问题。此外，通过考虑用户特征的多样性，个性化增强可以进一步增强对比学习在推荐器中用于设计图扰动操作的能力。通过根据特定的用户特征定制增强操作，我们可以更好地捕捉个人偏好。





