**5 CGI：通过信息瓶颈理论思考对比图结构学习的推荐系统**

[TOC]

---

# 概要
| | CGI |
| --- | --- |
| **题目** | Contrastive Graph Structure Learning via Information Bottleneck for Recommendation |
| **来源** | NeurIPS 2022 |
| **关键** | GCN；图对比学习；信息瓶颈Information Bottleneck |
| **代码** | [跳转中...](https://link.zhihu.com/?target=https%3A//github.com/weicy15/CGI) |
| **作者** | ![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1678800557282-d031dbbc-4a53-46d2-b7fe-93589bb95871.png#averageHue=%23efecea&clientId=u583ec1c3-1d69-4&from=paste&height=54&id=u9b798e8c&originHeight=68&originWidth=547&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=10370&status=done&style=none&taskId=u03f60495-5d9f-4b86-8d9a-68ea3dfdd3b&title=&width=437.6) |
| **摘要** | 用于推荐的图卷积网络 (GCN) 因其能够利用高阶邻居的能力而成为一个重要的研究课题。尽管取得了成功，但它们中的大多数都**受到少数活跃用户和流行项目带来的流行度偏差**的影响。此外，现实世界的用户-项目二部图**包含许多嘈杂的交互**，这可能会影响敏感的 GCN。**图对比学习**在解决推荐系统中上述挑战方面表现出了良好的性能。【引入图对比学习的作用】（图对比）大多数现有的工作通常执行**图增强**，**通过随机丢弃边/节点或依赖预定义的规则来创建原始图的多个视图，并且这些增强视图总是通过最大化它们的对应关系作为辅助任务**。然而，我们认为从这些 vanilla 方法生成的图结构可能是次优的，最大化它们的对应关系将迫使表示捕获与推荐任务无关的信息。【目前大多数图对比学习的缺陷】于是，本文我们提出了一种通过信息瓶颈进行图对比学习的框架(CGI) 进行推荐，它自适应地学习是否丢弃边或节点以端到端方式获得优化的图结构。此外，我们创新地将**信息瓶颈**引入到对比学习过程中，**避免捕获不同视图之间的不相关信息**，帮助丰富推荐的最终表示。<br />实验表明，所提模型明显优于基线。  |
| **评价** | 对比学习随机丢弃节点或边来生成增强视图的方法会破坏原有的图结构，丢弃一些重要的信息，或引入一些与推荐任务无关的信息，加重流行性偏差与噪声等影响。<br />在生成增强视图的时候，有策略性的丢弃节点或者边，以此来缓解流行性偏差问题；引入信息瓶颈到对比学习，保留最大的对下游推荐任务有用的信息，减少无用的信息，使聚焦于下游推荐任务；<br />本文主要的工作就是：<br />（1）**提出可学习图增强，通过学习是否删除边或节点，将原始二分图转换为相关视图，这些视图将以端到端的方式与下游推荐联合优化**。这些生成的视图可以有意地减少流行节点的影响，同时保留孤立节点的信息，从而有助于减轻流行偏差。<br />（2）**提出将不同的视图集成到下游推荐任务的密集表示中，进一步提高模型的鲁棒性**。通常，当来自不同视图的信息互补时，可以预期多视图表示学习方法可以提高下游性能。<br />本文的贡献总结如下：<br />(1)针对用户和物品的多视图表示学习，提出了CGI自适应删除节点和边来构造优化的图结构，为缓解流行度偏差提供了依据。<br />(2)为了高效丢弃与下游推荐无关的信息，创新地将信息瓶颈整合到多视图对比学习推荐过程中，并证明它能更好地缓解交互噪声。 |
| **笔记** | [NIPS2022｜CGI：基于信息瓶颈的对比图结构学习](https://zhuanlan.zhihu.com/p/583279425)<br />[论文阅读笔记：斯坦福大学SNAP团队《图信息瓶颈》—— Graph Information Bottleneck - DGSX - 博客园](https://cnblogs.com/shenchuguimo/p/14332100.html)<br />[NIPS &#124; GIB：图信息瓶颈理论定义“优秀”图表示](https://www.360doc.com/content/22/0419/11/79325180_1027225007.shtml) |
| **模型框架** | ![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679295605954-d6831427-1772-4ca5-9c6c-edfa48a787f9.png#averageHue=%23ede9e5&clientId=u45330e78-66eb-4&from=paste&height=430&id=u5ddb29f9&originHeight=537&originWidth=911&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=110264&status=done&style=none&taskId=uf2b81adf-c2d7-46ba-9d9d-6479feea845&title=&width=728.8) |
| **与相关论文的区别** | > 最近的工作SGL用辅助图CL任务补充了经典的监督推荐任务，它生成一个节点的多个视图，并最大化不同视图之间的一致性。</br> > 但与本文工作不同的是:</br>(1)**SGL通过随机删除边/节点来生成对比对**，而我们的工作采用了可学习的增宽器来优化生成的视图。</br>(2) SGL利用传统CL作为辅助任务，**最大化增强视图的一致性**，而我们建议鼓励增强视图与原始图之间的差异。</br>> AD-GCL[21]与本文CGI有一些相同的想法，但有根本的区别: 具体而言，AD-GCL专注于训练用于图级任务的自监督gnn。CGI旨在减轻节点级协同过滤(CF)的流行度偏差和交互噪声。此外，AD-GCL采取对抗性策略，旨在最大限度地使不同意见的最终陈述达成一致。相反CGI最小化了不同视图的相互信息，以捕获不同语义的协作信息。作者认为，本文研究是第一个利用IB原理来增强基于图表的推荐的研究。|


***

# 1.动机motivation
大多数基于gcn的推荐模型都存在以下两个局限性，其中对用户表现出的偏好的影响如图1所示。<br />i)**人气偏差。**<br />商品本身就有不同的顾客规模，这种不平衡可能会导致受欢迎程度偏差。在大多数推荐系统中，商品的顾客规模通常遵循长尾分布，即少数商品有大量顾客，而大多数商品的顾客很少。同样，大多数用户很少有交互。在多跳卷积过程中这种倾斜的数据分布将使基于gcn的模型容易偏向流行用户和物品，这可能会妨碍表示学习。<br />**ii)交互噪声。**<br />用户-道具交互通常包含噪音，特别是在只有隐式反馈的场景中(例如，点击和购买)。更具体地说，二部图中的这些噪声边并不一定与用户偏好一致，因为用户经常会在购买后错误地点击一些东西或发现一些无聊的东西。众所周知，基于gcn的模型容易受到输入图质量的影响，这意味着聚合误导性的邻域信息可能会导致次优性能。<br />图对比学习的最新进展已经确定了一种有效的训练方案，用于减轻基于图的任务的**流行度偏差和提高噪声的鲁棒性**，这激发了许多研究引入这种训练方案来增强推荐的表示学习。
> 然而现有的研究存在两个局限性。**首先，**大多数方法通过随机删除边/节点来执行数据增强，以改变图结构，变换嵌入以破坏节点表示，或依赖于预定义的规则。然而，在无监督的设置中，由这些vanilla方法创建的结构对于推荐任务来说可能不是最优的，而且也缺乏有说服力的理由来解释为什么随机删除的边/节点可以缓解流行偏差和交互噪声。就像图1中得到的表示N.1一样，由这些vanilla方法创建的结构可能会偏离最优区域。**其次，**大多数方法生成多个视图只是为了通过最大化这些视图之间节点表示的一致性来作为辅助任务，这可能会迫使不同视图中的用户或项目表示捕获与推荐任务无关的信息。例如，图1中得到的表示N.1包含了许多与实际偏好无关的信息。<br />因此，一个好的增广(如图1中的N.2)应该覆盖尽可能多的最优区域，同时尽可能小地减少无用信息。

为了解决上述局限性，本文提出基于信息瓶颈的对比图结构学习(CGI)进行推荐，它有两个关键组件：**可学习图增强和信息瓶颈对比学习。**
> 首先，我们提出了可学习图增强，学习是否删除边或节点，将原始二部图转换为相关视图，并与下游推荐以端到端的方式共同优化。因此，这些生成的视图可以有意地降低流行节点的影响，同时保留孤立节点的信息，从而有助于减轻流行偏差。这背后的直觉是，随机dropout会不考虑相应的节点度而不加区别地删除节点或边，而通过消息传递机制，GCNs更容易重建热门用户或物品的缺失信息，但很难重建那些连接较少的孤立节点，从而可能会过度强调那些高节点。这些生成的带有去偏信息的视图都被输入到基于gcn的推荐器中进行多视图表示学习，以提高抗流行度偏差的能力。
> 其次，我们提出将不同的视图集成为下游推荐任务的紧凑表示，这可以进一步提高模型的鲁棒性。通常，当来自不同视图的信息相互补充时，可以预期多视图表示学习方法可以提高下游性能。因此，我们认为，在传统的图对比学习中，简单地最大化相互信息可能会推动不同视图的表示来捕获与下游任务无关的信息。受信息瓶颈（IB）最新进展的启发，该进展期望表示为下游任务捕获最少的足够信息，我们利用IB原理最小化原始图和生成视图之间的相互信息，同时保持每个视图的下游推荐性能。通过这样做，可学习图增强可以学习尽可能地消除原始图中的噪声交互。此外，IB原则有助于不同视图的表示来捕获不同语义的协作信息。


<a name="E55oH"></a>
# 2.相关工作
<a name="tQKAR"></a>
## 2.1信息瓶颈理论
信息瓶颈(Information Bottleneck, IB)[23]是一种基于信息论的学习方法，该方法认为，如果所获得的表示丢弃了输入中对给定任务无用的信息，则可以提高下游任务的鲁棒性。_此外在多视图表示学习中使用了信息瓶颈原理[34,29,2]。_<br />形式上，给定原始数据X，标签为Y，IB是为了得到X的一个紧凑有效的Z表示，IB原理的目标如下:![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679297649562-4d748f72-3fd6-44ef-aaa7-250bf1561f71.png#averageHue=%23f6f4f1&clientId=u45330e78-66eb-4&from=paste&height=37&id=u302e4de8&originHeight=46&originWidth=263&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=3604&status=done&style=none&taskId=uf81d0747-ca6d-4438-a400-5fe799c95af&title=&width=210.4)<br />（最小化X和Z之间的互信息，最大化Y和Z的互信息，β为平衡系数）<br />[NIPS | GIB：图信息瓶颈理论定义“优秀”图表示](https://www.360doc.com/content/22/0419/11/79325180_1027225007.shtml)<br />[论文阅读笔记：斯坦福大学SNAP团队《图信息瓶颈》—— Graph Information Bottleneck - DGSX - 博客园](https://cnblogs.com/shenchuguimo/p/14332100.html)
<a name="NduBx"></a>
# 3.研究方法
<a name="ixwHn"></a>
## Learnable Multi-View Augmentation
将不同的学习子图与下游推荐相结合，分配不同的图卷积层，从而获得多视图用户和项目表示。我们详细介绍两种类型的可学习增强。<br />**Node-Dropping View**<br />图中受欢迎的用户或项目可能会影响数据分布，从而阻碍基于GCN的推荐。因此，我们在每一层进行可学习的节点删除，以屏蔽那些有影响的节点，并创建节点删除视图，可以表述为:<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679303089109-a13c375c-203e-43e0-a323-31a24fdab264.png#averageHue=%23faf8f6&clientId=u45330e78-66eb-4&from=paste&height=40&id=ue319b4d5&originHeight=50&originWidth=332&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=4798&status=done&style=none&taskId=u2b2921c5-833d-43c3-967b-10808998205&title=&width=265.6)![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679303232801-5aac3e41-0983-4100-bf9a-378cfe4c9a93.png#averageHue=%23f9f5f0&clientId=u45330e78-66eb-4&from=paste&height=31&id=ub14fab8d&originHeight=39&originWidth=36&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=907&status=done&style=none&taskId=u5be9410d-d405-489e-87e3-f1b283191aa&title=&width=28.8)是伯努利分布![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679407914891-7d46fec5-be94-46cf-b7d1-65310bf1b4e9.png#averageHue=%23fbf9f6&clientId=u23bce3dc-5553-4&from=paste&height=33&id=u1e12d7ec&originHeight=41&originWidth=68&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=1139&status=done&style=none&taskId=u878aa807-e33f-4eec-8556-3d7033efe8f&title=&width=54.4)![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679407928462-6f523fd8-1b2d-4d06-8e71-9bee777959b9.png#averageHue=%23f6f2ec&clientId=u23bce3dc-5553-4&from=paste&height=25&id=uc3f19f9b&originHeight=31&originWidth=111&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=2465&status=done&style=none&taskId=u97f3b993-5892-4ff7-a010-28f1e2f0ef4&title=&width=88.8)，用于判断是否保留节点Vi<br />_简单地删除所选节点及其所有连接将导致二部图结构发生巨大变化，从而产生不好的影响。_**因此本文不是删除所选节点，而是将所选节点v替换为其局部子图的表示，以模糊其原始表示并保留其相应的边。**对于节点v，我们对行走长度设置为ask的二部图G进行随机行走，然后将采样节点的均值池化作为v的局部子图的表示。<br />**Edge-Dropping View**<br />边降视图的目标是生成一个子图，过滤掉噪声边，并有意地降低流行节点对GCN的影响。与节点删除视图类似，我们通过可学习的边缘删除创建边缘删除视图:<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679407439048-7e222828-9c84-4a0c-9dcb-dddecf516e16.png#averageHue=%23faf8f6&clientId=u23bce3dc-5553-4&from=paste&height=42&id=u2f114beb&originHeight=52&originWidth=354&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=5206&status=done&style=none&taskId=u3bb83e0e-e7de-4cfb-8bbe-a5d1ca56251&title=&width=283.2)类似地![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679407611848-de404327-b36e-48e1-89c5-e40f9d90f236.png#averageHue=%23faf5f0&clientId=u23bce3dc-5553-4&from=paste&height=36&id=u765820ae&originHeight=45&originWidth=36&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=1039&status=done&style=none&taskId=u29962962-8729-44cb-8880-6cbc942379d&title=&width=28.8)也服从伯努利分布![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679407900789-23e694d4-5cfe-4c49-8969-6f4055b9d335.png#averageHue=%23faf6f2&clientId=u23bce3dc-5553-4&from=paste&height=33&id=u303bf495&originHeight=41&originWidth=177&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=2849&status=done&style=none&taskId=u75db51b8-a10b-431f-9f75-c89ab321bbc&title=&width=141.6)，用于表示边eij是否保留



采用多层感知机计算控制节点或边是否隐藏的参数w；![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679409056130-30ee3f6a-2b6c-4635-93e4-32683f5fc7d5.png#averageHue=%23f9f7f4&clientId=u23bce3dc-5553-4&from=paste&height=38&id=uc41d50ad&originHeight=48&originWidth=448&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=5737&status=done&style=none&taskId=u9e574dac-e09d-4694-9760-6270df8a11c&title=&width=358.4)<br />本文为了以端到端地优化多视图结构学习，将参数ρ从伯努利分布放宽为参数ω和独立随机变量ε的确定性函数即：![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679475062065-e6ef4a4c-1cb8-4999-8edc-b8e60057cfcf.png#averageHue=%23f6f3ef&clientId=u294290aa-bc5d-4&from=paste&height=25&id=uf1b9998c&originHeight=31&originWidth=428&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=4725&status=done&style=none&taskId=u776ba8f9-47b7-41e6-b390-e25aaa95559&title=&width=342.4)其中ε服从0~1的均匀分布，τ为正实数（反映温度？），σ为激活函数。τ＞0，该函数使用梯度![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679477676483-98eea778-6e35-4f94-a0fd-e3748547ea44.png#averageHue=%23f0ece7&clientId=u294290aa-bc5d-4&from=paste&height=35&id=u25d8d343&originHeight=44&originWidth=42&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=1277&status=done&style=none&taskId=u97030fc9-f49b-449c-b20d-46316af9ba9&title=&width=33.6)进行平滑，从而能够在训练期间有效优化节点丢弃视图和边缘丢弃视图的可学习建立。在推理时，我们以小于 0.5 的概率丢弃节点或边。
之后，再进行图卷积操作，获得这些视图上的用户和项目表示：![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679478441498-90437b40-28bf-4220-99d6-452b6959554a.png#averageHue=%23f4f1ed&clientId=u294290aa-bc5d-4&from=paste&height=38&id=udca2f3b4&originHeight=47&originWidth=716&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=8903&status=done&style=none&taskId=u0f3f9cc9-8d63-4018-b4f1-56437fe1d25&title=&width=572.8)还使用加权和分别构造最终的边视图和节点视图。
<a name="CB8A6"></a>
## Information Bottleneck Contrastive Learning
作者认为将可学习的增强过程和推荐过程结合在一起，但仅依靠推荐目标不能很好地指导dropout过程创建最优的增强视图。<br />因此本文采用Information-Bottleneck 原则来保留每个视图中最小的足够信息以进行下游推荐。具体来说，与传统的对比学习不同，我们鼓励增强视图和原始图的表示之间的差异，同时最大化与推荐任务相关的信息。于是可以获得更全面的多视图表示，并有效地丢弃噪声信息进行推荐。<br />优化目标为：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679485655022-0c311362-1c02-4a4d-a190-da16d7c18353.png#averageHue=%23f4f1ef&clientId=u294290aa-bc5d-4&from=paste&height=45&id=ue01e3397&originHeight=56&originWidth=259&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=4558&status=done&style=none&taskId=u7cdf48e0-0691-49e5-a31d-30e4d5f5f04&title=&width=207.2)<br />其中![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679486237579-99380f24-df24-4d43-95ff-016b4f1ae3b9.png#averageHue=%23f7f4f2&clientId=u294290aa-bc5d-4&from=paste&height=32&id=uf325797e&originHeight=40&originWidth=61&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=1155&status=done&style=none&taskId=ub3feb675-3828-41a1-8e67-9271c237ea5&title=&width=48.8)是增强视图表示的BPR损失，![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679486389165-5cc5f3bd-aab5-4353-a48a-7550a3c832cb.png#averageHue=%23edeae7&clientId=u294290aa-bc5d-4&from=paste&height=35&id=u15386c37&originHeight=44&originWidth=95&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=1663&status=done&style=none&taskId=uc9fb65d2-d186-4995-8202-575eaba8840&title=&width=76)表示来自两个相应视图的表示之间的互信息；<br />根据[25,19]，最小化InfoNCE损失[4]等价于最大化相应互信息的下界。因此，我们采用负InfoNCE来估计增强视图的表示与原始图之间的互信息，它由用户端和项端的互信息组成。形式上，对于用户侧互信息，将增强视图中同一用户的表示和原始图视为正对![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679487144005-56b5206f-2a5a-46e8-b69a-5e7a2a49c539.png#averageHue=%23f5f1ee&clientId=u294290aa-bc5d-4&from=paste&height=21&id=u8bdec349&originHeight=35&originWidth=297&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=4089&status=done&style=none&taskId=u98201e89-7288-47ca-ae4c-8a6e2c9d2e3&title=&width=179)，而增强视图中两个不同用户的表示和原始图表示为负对![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679487162300-cc25ce4d-870f-4801-94b4-f1769b783735.png#averageHue=%23f0ece8&clientId=u294290aa-bc5d-4&from=paste&height=24&id=u26161929&originHeight=30&originWidth=327&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=5967&status=done&style=none&taskId=uf4528e3b-cf3e-47df-b715-04c75d3f3ea&title=&width=261.6000061035156)<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679487657005-9c0fb48e-6236-42b9-a3f5-a185c735181e.png#averageHue=%23f6f4f3&clientId=u294290aa-bc5d-4&from=paste&height=71&id=u95f41b70&originHeight=89&originWidth=577&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=13838&status=done&style=none&taskId=ue96ad3d0-fef0-4ff5-883c-3086cc8a7ad&title=&width=461.6)<br />其中s(.)度量两个向量之间的相似度，本文设置为余弦相似度函数；![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679490061576-ee3ac19f-20b9-4da6-befd-7d6c87940693.png#averageHue=%23f5f2ef&clientId=u294290aa-bc5d-4&from=paste&height=22&id=u954899ff&originHeight=27&originWidth=24&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=479&status=done&style=none&taskId=u2d09c929-eea2-4d12-ad87-c10e865ad39&title=&width=19.2)是类似温度的超参数；<br />类似地，从物品侧度量获得互信息![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679490216044-7bc8ca5f-9534-4d7b-b301-a96c2ff1ece9.png#averageHue=%23ece8e4&clientId=u294290aa-bc5d-4&from=paste&height=30&id=u75c59b35&originHeight=37&originWidth=113&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=1983&status=done&style=none&taskId=u36d0a3e7-3c59-41ee-bdc2-26ddfc23ecd&title=&width=90.4)，<br />结合最终获得整体互信息![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679490253437-b1cce38e-a68e-47e8-a85b-26d5abcc9d97.png#averageHue=%23ece8e5&clientId=u294290aa-bc5d-4&from=paste&height=27&id=udafe9426&originHeight=34&originWidth=400&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=4341&status=done&style=none&taskId=u98d64d8e-7185-4736-9fca-49cc854e4ff&title=&width=320)
<a name="brA70"></a>
## Optimization
为了获得全面的多视图表示，利用**两个参数化网络**学习同时创建Node-Dropping视图和Edge-Dropping视图。<br />为了全面探索两个视图以获得更好的推荐，联合优化视图的推荐任务和自监督IB对比学习:<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679491157753-aaa2ad1c-4c6f-4fdd-8c60-09df3a7c30fe.png#averageHue=%23f4f1ef&clientId=u294290aa-bc5d-4&from=paste&height=34&id=u5524e1d6&originHeight=43&originWidth=798&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=10681&status=done&style=none&taskId=ue4f8a9bf-6ece-4a59-9ebf-ca480de15dd&title=&width=638.4)<br />（想起上学期DrectAU直接以对齐性和均匀性为目标进行优化学习）<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679491303337-e8ac0455-ff0f-474d-8387-6d098c00aa37.png#averageHue=%23f5f2ee&clientId=u294290aa-bc5d-4&from=paste&height=15&id=uc3c179e8&originHeight=44&originWidth=159&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=2898&status=done&style=none&taskId=u893f24ba-cec9-4b36-bb26-10aa0bc2fe8&title=&width=54)和![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679491312630-27e817b0-b2bb-43a5-8e12-7543f2fa0af4.png#averageHue=%23f5f2ee&clientId=u294290aa-bc5d-4&from=paste&height=15&id=ud135dc67&originHeight=44&originWidth=159&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=2898&status=done&style=none&taskId=u123da1d0-1c19-4d93-bdd3-634af733bce&title=&width=54)分别是Node-Dropping视图和Edge-Dropping视图的推荐目标，λ和β分别是控制IB对比学习任务和L2正则化效果强度的超参数。

### 关于互信息上限的定理：
将学习到的增强视图表示为![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679491845595-b5f4d3dc-ffce-4895-879d-cd81edae3475.png#averageHue=%23ebe7e2&clientId=u294290aa-bc5d-4&from=paste&height=27&id=u44d4606e&originHeight=34&originWidth=25&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=708&status=done&style=none&taskId=u63e9594d-e31c-4988-bfe6-47bfc0da370&title=&width=20)，噪声图结构为![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679492454386-7d217440-db6d-46bd-895e-6150cf428cab.png#averageHue=%23e9e4de&clientId=u294290aa-bc5d-4&from=paste&height=23&id=ub3bcca64&originHeight=29&originWidth=27&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=727&status=done&style=none&taskId=u85714563-8245-4f97-bf7f-e16c8b53cac&title=&width=21.6)，下游推荐任务为![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679492540142-45f43335-f8f7-4718-bdf5-c2c5dc50d625.png#averageHue=%23f3efeb&clientId=u294290aa-bc5d-4&from=paste&height=24&id=uf6c6b8ac&originHeight=30&originWidth=62&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=1195&status=done&style=none&taskId=uafc4b26e-7aa3-48fc-8d66-0e961cba5c8&title=&width=49.6)，假设![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679492454386-7d217440-db6d-46bd-895e-6150cf428cab.png#averageHue=%23e9e4de&clientId=u294290aa-bc5d-4&from=paste&height=23&id=VUgM2&originHeight=29&originWidth=27&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=727&status=done&style=none&taskId=u85714563-8245-4f97-bf7f-e16c8b53cac&title=&width=21.6)与![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679492645289-2e40dd5e-ab63-4f05-8438-30d10b2ced67.png#averageHue=%23f3efeb&clientId=u294290aa-bc5d-4&from=paste&height=24&id=u87236187&originHeight=30&originWidth=62&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=1195&status=done&style=none&taskId=uc5e892e3-7409-4909-a3d9-7710b5e57de&title=&width=49.6)无关，互信息![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679492775188-f21f0690-4b1e-4f91-b7d4-3a484f39bfdf.png#averageHue=%23f5f2ee&clientId=u294290aa-bc5d-4&from=paste&height=9&id=uadf82590&originHeight=41&originWidth=383&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=5279&status=done&style=none&taskId=u89cbf814-520b-4ed6-abc1-bb8458dc615&title=&width=81)的上限为![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679492819906-e84fd211-9628-40a5-9439-fecbdb804d6e.png#averageHue=%23f5f2ee&clientId=u294290aa-bc5d-4&from=paste&height=21&id=u20a06fe1&originHeight=41&originWidth=383&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=5279&status=done&style=none&taskId=u5b3e0b91-d54f-4009-9e85-a448def8769&title=&width=195)：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679492740257-43475871-0f2d-47a7-9af8-c7a4c21296b5.png#averageHue=%23f5f2ee&clientId=u294290aa-bc5d-4&from=paste&height=33&id=u36bd7838&originHeight=41&originWidth=383&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=5279&status=done&style=none&taskId=u9aa686f9-6a71-4db7-b080-026bea70eb1&title=&width=306.4)

![image.png](https://cdn.nlark.com/yuque/0/2023/png/29360045/1679492937562-389eb25d-73a0-4910-86ba-3f9e04894b64.png#averageHue=%23f3f0ed&clientId=u294290aa-bc5d-4&from=paste&height=498&id=uf793c4f5&originHeight=623&originWidth=1142&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=173665&status=done&style=none&taskId=u99c00f23-b4fb-4b4a-b8da-63c2b783a5d&title=&width=913.6)
<a name="tH3d7"></a>
# 4.实验

<a name="Kqprs"></a>
# 5.总结

