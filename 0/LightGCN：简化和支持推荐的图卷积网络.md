<a name="ckvdw"></a>
# Abstract
图卷积网络（Graph Convolution Network）（GCN）已成为协同过滤的最新技术。然而，其推荐有效性的原因尚不清楚。现有的使 GCN 适应推荐的工作缺乏对 GCN 的彻底消融分析，GCN 最初是为图分类任务而设计的，并配备了许多神经网络操作。然而，我们凭经验发现 GCN 中最常见的两种设计——特征转换和非线性激活——对协同过滤的性能贡献不大。更糟糕的是，包括它们会增加训练的难度并降低推荐性能。<br />在这项工作中，我们旨在简化 GCN 的设计，使其更简洁，更适合推荐。我们提出了一个名为 LightGCN 的新模型，它只包括 GCN 中最重要的组件——邻域聚合——用于协同过滤。具体来说，LightGCN 通过在用户-项目交互图上线性传播它们来学习用户和项目嵌入，并使用在所有层学习到的 embedding的加权和作为最终 embedding。这种简单、线性和整洁的模型更容易实现和训练，与神经图协同过滤 (NGCF)（基于 GCN 的最先进的推荐模型）相比，表现出显着的改进（平均相对改进约 16.0%） — 在完全相同的实验设置下。从分析和经验的角度对简单LightGCN的合理性进行了进一步的分析。我们的实现在 TensorFlow1 和 PyTorch2 中都可用。<br />协同过滤：[https://blog.csdn.net/qq_45301231/article/details/122089100](https://blog.csdn.net/qq_45301231/article/details/122089100)<br />GCN：[https://blog.csdn.net/qq_42325947/article/details/115503956](https://blog.csdn.net/qq_42325947/article/details/115503956)<br />[https://blog.csdn.net/weixin_43702653/article/details/123850998](https://blog.csdn.net/weixin_43702653/article/details/123850998)（GCN和GNN有不同）<br />embedding：[https://www.zhihu.com/question/38002635](https://www.zhihu.com/question/38002635)
<a name="n6qMq"></a>
# Introduction
为了减轻网络上的信息过载，推荐系统已被广泛用于执行个性化信息过滤[7,45,46]。推荐系统的核心是预测用户是否会与某个项目进行交互，例如点击、评分、购买以及其他形式的交互。因此，专注于利用过去的用户-项目交互来实现预测的协同过滤 (CF) 仍然是实现有效个性化推荐的一项基本任务 [10, 19, 28, 39]。 CF 最常见的范例是学习潜在特征（也称为嵌入）来表示用户和项目，并根据嵌入向量进行预测 [6, 19]。矩阵分解是一种早期的此类模型，它直接将用户的单个 ID 投影到她的嵌入 [26]。后来，一些研究发现，用她的交互历史作为输入来增加用户 ID 可以提高嵌入的质量。例如，SVD++ [25] 展示了用户交互历史在预测用户数值评分方面的好处，神经注意力项目相似性 (NAIS) [18] 区分了项目在交互历史中的重要性，并显示了预测项目排名的改进。鉴于用户-项目交互图，这些改进可以看作来自使用用户的子图结构——更具体地说，是她的单跳邻居——来改进嵌入学习。<br />为了加深对具有高跳邻居的子图结构的使用，Wang 等人。 [39] 最近提出了 NGCF，并为 CF 实现了最先进的性能。它从图卷积网络 (GCN) [14, 23] 中汲取灵感，遵循相同的传播规则来细化 embedding：特征变换、邻域聚合和非线性激活。尽管 NGCF 已显示出可喜的结果，但我们认为它的设计相当繁重和繁重——许多操作是直接从 GCN 继承而来的，没有任何理由。因此，它们不一定对 CF 任务有用。具体来说，GCN 最初是针对属性图上的节点分类提出的，每个节点都有丰富的属性作为输入特征；而在CF的用户-项目交互图中，每个节点（用户或项目）仅由一个单热ID描述，除了作为标识符之外没有具体的语义。在这种情况下，给定 ID  embedding作为输入，执行多层非线性特征变换——这是现代神经网络 [16] 成功的关键——不会带来任何好处，反而会增加模型训练的难度。<br />为了验证我们的想法，我们对 NGCF 进行了广泛的消融研究。通过严格的对照实验（在相同的数据拆分和评估协议上），我们得出结论，从 GCN 继承的两个操作——特征转换和非线性激活——对 NGCF 的有效性没有贡献。更令人惊讶的是，删除它们会显着提高准确性。这反映了在图神经网络中添加对目标任务无用的操作的问题，这不仅没有带来任何好处，反而降低了模型的有效性。受这些实证发现的启发，我们提出了一个名为 LightGCN 的新模型，其中包括 GCN 中最重要的组件——邻域聚合——用于协同过滤。具体来说，在将每个用户（项目）与 ID 嵌入相关联后，我们在用户-项目交互图上传播嵌入以改进它们。然后，我们将在不同传播层学习的嵌入与加权和相结合，以获得用于预测的最终 embedding。整个模型简单而优雅，不仅更容易训练，而且比 NGCF 和其他最先进的方法如 Mult-VAE [28] 获得了更好的经验性能。<br />总而言之，这项工作做出了以下主要贡献：

   - 我们凭经验表明，GCN 中的两种常见设计，特征转换和非线性激活，对协同过滤的有效性没有积极影响。
   - 我们提出了 LightGCN，它通过仅包含 GCN 中最重要的组件进行推荐，大大简化了模型设计。
   - 我们通过遵循相同的设置经验性地将 LightGCN 与 NGCF 进行比较，并展示了实质性的改进。从技术和经验的角度对 LightGCN 的合理性进行了深入的分析。
<a name="s9O9T"></a>
# PRELIMINARIES
我们首先介绍 NGCF [39]，这是一个具有代表性和最先进的 GCN 推荐模型。然后，我们对 NGCF 进行消融研究，以判断 NGCF 中每个操作的有用性。本节的新颖贡献是表明 GCN 中的两种常见设计，特征转换和非线性激活，对协同过滤没有积极影响。<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656315479069-5cb4daca-2a15-4c5f-8ae5-8e489bb5763d.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=265&id=u02c835d6&originHeight=291&originWidth=716&originalType=binary&ratio=1&rotation=0&showTitle=false&size=51596&status=done&style=none&taskId=u9387b334-9a2a-4ee3-a40a-2eae2895a0e&title=&width=650.9090768010166)
<a name="E3eAl"></a>
## NGCF Brief
在初始步骤中，每个用户和项目都与一个 ID embedding相关联。令 e(0) u 表示用户 u 的 ID embedding，e(0) i 表示项目 i 的 ID embedding。然后 NGCF 利用 useritem 交互图将embedding传播为：<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656315681061-16e9d347-1b58-47b8-bf3b-d5b86076e020.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=143&id=ubdd97f62&originHeight=157&originWidth=751&originalType=binary&ratio=1&rotation=0&showTitle=false&size=29808&status=done&style=none&taskId=u82a97398-4b03-4c4b-92f2-c6ae3d1792c&title=&width=682.7272579295579)[<br />](https://blog.csdn.net/qq_42325947/article/details/115503956)分别表示经过 k 层传播后用户 u 和项目 i 的更新后的 embedding，σ 是非线性激活函数，Nu 表示与用户 u 交互的项目集，Ni 表示与项目 i 交互的用户集，W1 和 W2 是可训练的权重矩阵，用于在每一层进行特征变换。通过传播 L 层，NGCF 获得 L + 1 个embedding来描述用户 (e(0) u , e(1) u , ..., e(L) u ) 和项目 (e(0) i , e( 1) i , ..., e(L) i )。然后它连接这些 L + 1 个 embedding以获得最终的用户 embedding和项目 embedding，使用内积生成预测分数。<br />NGCF 很大程度上遵循标准 GCN [23]，包括使用非线性激活函数 σ(·) 和特征变换矩阵 W1 和 W2。然而，我们认为这两个操作对于协同过滤没有那么有用。在半监督节点分类中，每个节点都具有丰富的语义特征作为输入，例如论文的标题和抽象词。因此执行多层非线性变换有利于特征学习。然而，在协同过滤中，用户项交互图的每个节点只有一个 ID 作为输入，没有具体的语义。在这种情况下，执行多个非线性变换将无助于学习更好的特征；更糟糕的是，它可能会增加训练的难度。在下一小节中，我们提供了关于这一论点的经验证据。
<a name="Bc765"></a>
## Empirical Explorations on NGCF 
我们对 NGCF 进行消融研究，以探索非线性激活和特征转换的影响。我们使用 NGCF作者发布的代码，在相同的数据拆分和评估协议上运行实验，以尽可能保持比较公平。由于 GCN 的核心是通过传播来细化 embedding，因此我们对相同embedding大小下的embedding质量更感兴趣。因此，我们将获得最终embedding的方式从连接（即，e* u = e(0) u ∥··· ∥e(L) u ）更改为求和（即，e* u = e(0) u + ···+e(L)u)。请注意，这种变化对 NGCF 的性能影响不大，但使以下消融研究更能表明 GCN 改进的embedding质量。<br />。。。。。。<br />对于这三个变体，我们保持所有超参数（例如，学习率、正则化系数、辍学率等）与 NGCF 的最佳设置相同。我们在表 1 中报告了 Gowalla 和 Amazon-Book 数据集上的 2 层设置的结果。可以看出，移除特征转换（即 NGCF-f）会导致在所有三个数据集上对 NGCF 的一致改进。相反，去除非线性激活不会对准确性产生太大影响。但是，如果我们在去除特征变换（即NGCF-fn）的基础上去除非线性激活，性能会显着提高。根据这些观察，我们得出以下结论：<br />(1) 添加特征变换会对 NGCF 产生负面影响，因为在 NGCF 和 NGCF-n 模型中移除它都会显着提高性能；<br /> (2) 添加非线性激活在包含特征变换时影响较小，但在禁用特征变换时会产生负面影响。 <br />(3) 总体而言，特征转换和非线性激活对 NGCF 产生了相当大的负面影响，因为通过同时移除它们，NGCF-fn 表现出比 NGCF 有很大的改进（召回率相对提高了 9.57%）。
<a name="hBBB2"></a>
# METHOD
<a name="C31Cv"></a>
## LightGCN
GCN 的基本思想是通过在图上平滑特征来学习节点的表示 [23, 40]。为了实现这一点，它迭代地执行图卷积，即将邻居的特征聚合为目标节点的新表示。这种邻域聚合可以抽象为：e(k+1) u = AGG(e(k) u , {e(k) i : i ∈ Nu })。 (2) AGG 是一个聚合函数——图卷积的核心——它考虑了目标节点及其相邻节点的第 k 层表示。许多工作都指定了 AGG，例如 GIN [42] 中的加权和聚合器、GraphSAGE [14] 中的 LSTM 聚合器和 BGNN [48] 中的双线性交互聚合器等。但是，大多数工作都与特征转换或非线性激活有关与 AGG 功能。尽管它们在具有语义输入特征的节点或图分类任务上表现良好，但它们对于协同过滤可能是繁重的（参见第 2.2 节中的初步结果）。
<a name="NQhOj"></a>
### 3.1.1
光图卷积 (LGC)。在 LightGCN 中，我们采用了简单的加权和聚合器，放弃了使用特征变换和非线性激活。 LightGCN 中的图卷积操作（又名传播规则 [39]）定义为：<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656319074878-6d250a8d-3e06-446c-ae7f-754d1942dda9.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=140&id=u08db3c4e&originHeight=154&originWidth=351&originalType=binary&ratio=1&rotation=0&showTitle=false&size=12221&status=done&style=none&taskId=u99b42443-d5de-427d-86c8-644696f46c2&title=&width=319.0909021748)<br />归一化可以避免嵌入的规模随着图卷积操作的增加而增加；其他选择也可以在这里应用，例如 L1 范数，而经验上我们发现这种对称归一化具有良好的性能（参见第 4.4.2 节中的实验结果）。<br />值得注意的是，在 LGC 中，我们只聚合连接的邻居，而不集成目标节点本身（即自连接）。这与大多数现有的图卷积操作 [14, 23, 36, 39, 48] 不同，后者通常聚合扩展的邻居并需要专门处理自连接。将在下一小节中介绍的层组合操作基本上捕获了与自连接相同的效果。因此，LGC 中不需要包含自连接。
<a name="gUllG"></a>
### 3.1.2
层组合和模型预测。在 LightGCN 中，唯一可训练的模型参数是第 0 层的嵌入，即所有用户的 e(0) u 和所有项目的 e(0) i。当给出它们时，可以通过等式 (3) 中定义的 LGC 计算更高层的嵌入。在 K 层 LGC 之后，我们进一步组合在每一层获得的embedding，以形成用户（一个项目）的最终表示：<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656320293942-6a85fa75-97e1-4a76-8143-e7b93199517c.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=91&id=uf2f22bd0&originHeight=100&originWidth=503&originalType=binary&ratio=1&rotation=0&showTitle=false&size=9315&status=done&style=none&taskId=ubc758f05-38a3-42aa-b058-7e9d0c837b8&title=&width=457.27271736160804)<br />其中 αk ≥ 0 表示第 k 层embedding在构成最终embedding中的重要性。它可以被视为要手动调整的超参数，也可以被视为要自动优化的模型参数（例如，注意力网络 [3] 的输出）。在我们的实验中，我们发现将 αk 统一设置为 1/(K + 1) 通常会带来良好的性能。因此，我们没有设计特殊的组件来优化 αk，以避免不必要地使 LightGCN 复杂化并保持其简单性。我们执行层组合以获得最终表示的原因有三个。<br /> (1) 随着层数的增加，embedding将被过度平滑[27]。因此，简单地使用最后一层是有问题的。<br /> (2) 不同层的embedding捕获不同的语义。例如，第一层对具有交互的用户和项目强制平滑，第二层平滑在交互项目（用户）上重叠的用户（项目），更高层捕获更高阶的接近度[39]。因此，将它们结合起来将使表示更加全面。 <br />(3) 将不同层的embedding与加权和相结合，捕捉到图卷积与自连接的效果，这是 GCN 中的一个重要技巧（证明见第 3.2.1 节）。<br />模型预测被定义为用户和项目最终表示的内积：![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656320775637-85dbc338-d8c8-47ba-80aa-09b3ce805247.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=34&id=ua3ce17b6&originHeight=37&originWidth=131&originalType=binary&ratio=1&rotation=0&showTitle=false&size=2136&status=done&style=none&taskId=u0f5c8599-4773-49ab-a3f8-924b87e24a0&title=&width=119.09090650968321) 用作推荐生成的排名分数。
<a name="Bkx6A"></a>
### 3.1.3
矩阵形式。我们提供 LightGCN 的矩阵形式，以方便与现有模型的实现和讨论。设用户-项目交互矩阵为 R ∈ RM×N，其中 M 和 N 分别表示用户和项目的数量，每个条目 Rui 为 1，如果已与项目 i 交互，则为 0。然后我们得到邻接矩阵用户项目图为![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656320925651-92301619-c7ef-4445-a2d8-ceec2e6d300f.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=56&id=ue9e3a9f1&originHeight=62&originWidth=134&originalType=binary&ratio=1&rotation=0&showTitle=false&size=3260&status=done&style=none&taskId=u1b2ee23d-fb58-4c1f-bc65-1dca92a597f&title=&width=121.81817917784389)<br />设第 0 层嵌入矩阵为 E(0) ∈ R(M+N)×T，其中 T 是嵌入大小。那么我们可以得到LGC的矩阵等价形式为：<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656321176524-6cf9a710-d214-43a0-b046-e93dee615672.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=31&id=u9540580b&originHeight=34&originWidth=232&originalType=binary&ratio=1&rotation=0&showTitle=false&size=3925&status=done&style=none&taskId=u7c2e6a8a-40a2-4bd9-af26-2ffc30cd215&title=&width=210.90908633775956)<br />其中 D 是一个 (M+N)×(M+N) 对角矩阵，其中每个条目 Dii 表示邻接矩阵 A（也称为度矩阵）的第 i 行向量中非零条目的数量。最后，我们得到用于模型预测的最终嵌入矩阵：<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656321269448-02faa21e-f31d-43da-abef-7e55146a5c68.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=40&id=u37c309d2&originHeight=44&originWidth=458&originalType=binary&ratio=1&rotation=0&showTitle=false&size=5787&status=done&style=none&taskId=ub5f39608-7e15-469b-a7d2-002b9879067&title=&width=416.36362733919776)
<a name="M2ead"></a>
## Model Analysis
进行模型分析以证明 LightGCN 简单设计背后的合理性。首先我们讨论与 Simplified GCN (SGCN) [40] 的连接，这是一种最近将自连接集成到图卷积中的线性 GCN 模型；分析表明，通过层合并，LightGCN 包含了自连接的影响，因此 LightGCN 不需要在邻接矩阵中添加自连接。然后我们讨论与神经预测的近似个性化传播 (APPNP) [24] 的关系，这是最近的 GCN 变体，通过个性化 PageRank [15] 的启发来解决过度平滑问题；该分析显示了 LightGCN 和 APPNP 之间的潜在等价性，因此我们的 LightGCN 在具有可控过平滑的远程传播方面具有相同的优势。最后，我们分析了第二层 LGC，以展示它如何平滑用户与她的二阶邻居，从而为 LightGCN 的工作机制提供更多见解。
<a name="AX0EW"></a>
### 3.2.1 与SGCN的关系。
在 [40] 中，作者认为 GCN 对节点分类不必要的复杂性并提出了 SGCN，它通过消除非线性并将权重矩阵折叠为一个权重矩阵来简化 GCN。 SGCN中的图卷积定义为4：![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656327910796-d0426a58-7002-4466-a497-388fff2665f8.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=31&id=u2c887b3d&originHeight=34&originWidth=367&originalType=binary&ratio=1&rotation=0&showTitle=false&size=5534&status=done&style=none&taskId=u93e0f552-d292-4036-88e4-31340b58b02&title=&width=333.63635640499035)<br />其中 I ∈ R(M+N)×(M+N) 是一个单位矩阵，它被添加到 A 上以包含自连接。在下面的分析中，为了简单起见，我们省略了 (D + I)− 1/2 项，因为它们只重新缩放embedding。在 SGCN 中，最后一层得到的embedding用于下游预测任务，可以表示为：<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656328039665-715e0513-d35d-493c-b72a-e3d501bb0846.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=95&id=u80b26b51&originHeight=105&originWidth=529&originalType=binary&ratio=1&rotation=0&showTitle=false&size=14100&status=done&style=none&taskId=uf79753ce-8383-4ad9-ac67-e219bba9f9a&title=&width=480.9090804856673)
<a name="evU8G"></a>
### 3.2.2 Relation with APPNP.
在最近的一项工作 [24] 中，作者将 GCN 与 Personalized PageRank [15] 联系起来，启发他们提出了一个名为 APPNP 的 GCN 变体，它可以在没有过度平滑的风险的情况下进行长距离传播。受 Personalized PageRank 中的瞬移设计的启发，APPNP 用起始特征（即第 0 层embedding）补充每个传播层，这可以平衡保持局部性的需求（即保持靠近根节点以减轻过度平滑）并利用来自大社区的信息。 APPNP中的传播层定义为：<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656328233006-31d2e59b-43dd-4420-af01-d5eaa6b167ba.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=38&id=u8b84661c&originHeight=42&originWidth=277&originalType=binary&ratio=1&rotation=0&showTitle=false&size=4596&status=done&style=none&taskId=ud3807d00-8d60-47a3-9164-0b0366db085&title=&width=251.81817636016984)<br />其中β是控制在传播中保留起始特征的瞬移概率，A表示归一化邻接矩阵。在 APPNP 中，最后一层用于最终预测，即<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656328292764-e90476f3-bf48-4c02-bb8f-8c84587853e6.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=107&id=u9e531540&originHeight=118&originWidth=630&originalType=binary&ratio=1&rotation=0&showTitle=false&size=20234&status=done&style=none&taskId=u9d3fd0bf-70a5-468d-bb25-08f23a4126f&title=&width=572.7272603137437)<br />与3.1.3中等式对齐，我们可以看到通过相应地设置 αk，LightGCN 可以完全恢复 APPNP 使用的预测嵌入。因此，LightGCN 在对抗过度平滑方面具有 APPNP 的优势——通过正确设置 α，我们允许使用较大的 K 进行具有可控过度平滑的远程建模。另一个小的区别是 APPNP 在邻接矩阵中添加了自连接。然而，正如我们之前所展示的，由于不同层的加权和，这是多余的。
<a name="SegD2"></a>
### 3.2.3 Second-OrderEmbedding Smoothness.
由于 LightGCN 的线性和简单性，我们可以更深入地了解它如何平滑embedding。这里我们分析一个 2 层的 LightGCN 来证明它的合理性。以用户端为例，直观上，第二层平滑了交互项目上有重叠的用户。更具体地说，我们有：<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656328733307-d572d2c5-2b5a-46a0-bc60-f28005f844dc.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=59&id=u1d60406a&originHeight=65&originWidth=577&originalType=binary&ratio=1&rotation=0&showTitle=false&size=11507&status=done&style=none&taskId=u5d06e055-4169-4d1b-8fae-e3c3b69c9e0&title=&width=524.5454431762382)<br />我们可以看到，如果另一个用户 v 与目标用户 u 进行了共同交互，则 v 在 u 上的平滑强度通过系数来衡量（否则为 0）：![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1656328780241-4850526d-f44e-4861-bf6d-b5d3039a522e.png#clientId=u0f1c7cc1-7a06-4&from=paste&height=55&id=u1bc2bd07&originHeight=61&originWidth=372&originalType=binary&ratio=1&rotation=0&showTitle=false&size=6195&status=done&style=none&taskId=u8109345e-df79-42ef-9814-e6f25e352a5&title=&width=338.1818108519248)<br />这个系数比较容易解释：二阶邻居 v 对 u 的影响由 1) 共同交互项目的数量决定，越多越大； 2）共同交互项目的受欢迎程度，越低的受欢迎程度（即更能表明用户个性化偏好）越大； 3）v的活跃度，越低活跃度越大。这种可解释性很好地满足了 CF 在测量用户相似性方面的假设 [2, 37]，并证明了 LightGCN 的合理性。由于 LightGCN 的对称公式，我们可以在 item 端得到类似的分析。
<a name="aBH9a"></a>
## Model Training
LightGCN 的可训练参数只是第 0 层的embedding，即 Θ = {E(0)}；换句话说，模型复杂度与标准矩阵分解（MF）相同。我们采用Bayesian Personalized Ranking (BPR) loss（BPR）损失[32][https://blog.csdn.net/sigmeta/article/details/80517828](https://blog.csdn.net/sigmeta/article/details/80517828)，这是一种成对损失，鼓励观察到的条目的预测高于其未观察到的对应物：
