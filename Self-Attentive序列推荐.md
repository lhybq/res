Self-Attentive Sequential Recommendation，基于自注意力机制的序列推荐
<a name="gXfJv"></a>
# Abstract
序列的动态特征是许多现代推荐系统的一个关键特性，它们试图根据用户最近的操作来捕捉用户活动的“上下文”。为了捕捉这样的模式，两种方法已经被提出:马尔可夫链(MCs)和循环神经网络(rnn)。马尔可夫链假设用户的下一个操作可以根据他们的最后一个(或最后几个)操作来预测，而rnn原则上允许发现更长期的隐藏的语义。<br />一般来说，基于mc的方法在及其稀疏的数据集中表现最好，在这种数据集中模型简约是关键，而rnn在可以负担得起更高复杂度的模型的更为的密集数据集中表现更好。<br />我们的工作目标是平衡这两个目标，通过提出一个基于自我注意的序列模型(SASRec)，它允许我们捕捉长期语义(像RNN)，但使用注意机制，使其预测基于相对较少的最近的行动(像MC)。在每个时间步骤，SASRec试图从用户的操作历史中识别哪些项目是“相关的”，并使用它们来预测下一个项目。大量的实证研究表明，我们的方法在稀疏和密集数据集上都优于各种最先进的顺序模型(包括基于MC/CNN/ rnn的方法)。此外，该模型的效率比基于CNN/ rnn的可比模型高出一个数量级。注意力权重的可视化也显示了我们的模型如何自适应地处理不同密度的数据集，并揭示了活动序列中有意义的模式。
<a name="IMx5V"></a>
# Introduction
序列推荐系统的目标是将用户行为(基于历史活动)的个性化模型与基于用户最近行为的“上下文”概念结合起来。从序列动力学中捕获有用的模式是一项挑战，主要是因为输入空间的维度会随着过去用作上下文的动作的数量呈指数级增长。因此，序列推荐的研究主要关注如何简洁地捕捉这些高阶动态信息。<br />马尔可夫链(Markov Chains, mc)是一个经典的例子，它假设下一个操作只取决于前一个操作(或前几个操作)，并已成功地用于描述建议[1]的短期项目转换。另一项工作是使用循环神经网络(rnn)通过一个隐藏状态来总结所有以前的操作，这个隐藏状态用于预测下一个操作[2]。<br />这两种方法，虽然在特定情况下都很强大，但在某种程度上限制了特定类型的数据。基于mc的方法，通过做出强大的简化假设，在高稀疏度设置中表现良好，但可能无法捕捉更复杂场景的复杂动态。相反，rnn虽然具有表达能力，但需要大量数据(尤其是密集数据)才能超过简单的baseline。<br />![简图显示SASRec的培训过程。在每个时间步骤中，该模型会考虑之前的所有项，并使用注意力来“关注”与下一个操作相关的项。](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657591097710-8b0a721d-0542-4716-b07c-d3e1fb9fbf90.png#clientId=ubd68cdfe-2e47-4&from=paste&height=389&id=u062e7208&originHeight=391&originWidth=431&originalType=binary&ratio=1&rotation=0&showTitle=true&size=49910&status=done&style=none&taskId=u1d8a77d0-0049-42fd-ba2e-1712cdf317e&title=%E7%AE%80%E5%9B%BE%E6%98%BE%E7%A4%BASASRec%E7%9A%84%E5%9F%B9%E8%AE%AD%E8%BF%87%E7%A8%8B%E3%80%82%E5%9C%A8%E6%AF%8F%E4%B8%AA%E6%97%B6%E9%97%B4%E6%AD%A5%E9%AA%A4%E4%B8%AD%EF%BC%8C%E8%AF%A5%E6%A8%A1%E5%9E%8B%E4%BC%9A%E8%80%83%E8%99%91%E4%B9%8B%E5%89%8D%E7%9A%84%E6%89%80%E6%9C%89%E9%A1%B9%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9D%A5%E2%80%9C%E5%85%B3%E6%B3%A8%E2%80%9D%E4%B8%8E%E4%B8%8B%E4%B8%80%E4%B8%AA%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E7%9A%84%E9%A1%B9%E3%80%82&width=429 "简图显示SASRec的培训过程。在每个时间步骤中，该模型会考虑之前的所有项，并使用注意力来“关注”与下一个操作相关的项。")<br />最近，一种新的序列模型transformer实现了最先进的性能和效率，用于机器翻译任务[3]。与现有的使用卷积或循环模块的顺序模型不同，Transformer纯粹基于一种被称为“自我注意”的注意机制，这种注意机制效率很高，能够发现句子中单词之间的句法和语义模式。<br />受此启发，我们试图将自我注意机制应用于序列推荐问题。我们希望这个想法能够解决上面提到的两个问题，一方面能够从过去的所有相关行动中提取“上下文信息”(如rnn)，但另一方面能够根据少量的行动(如mc)构建预测框架。具体而言，我们构建了一个基于Self-Attention的序列推荐模型(SASRec)，该模型在每个时间步骤中自适应地为之前的条目分配权重(图1)。<br />所提出的模型在几个基准数据集上显著优于最先进的基于MC/CNN/ rnn的顺序推荐方法。**特别地，我们将性能作为数据集稀疏性的函数来检查，其中模型性能与上面描述的模式密切相关。由于自我注意机制，SASRec倾向于考虑对密集数据集的长期依赖，同时专注于更近期的稀疏数据集活动。这对于自适应处理密度变化的数据集至关重要。（意思是模型的性能与数据集的稀疏性密切相关，性能可以看作是数据集稀疏性的函数；而所提模型可以自适应的处理这种数据集稀疏性变化的情况。）**<br />此外，SASRec的核心部分(即自注意力模块)适用于并行加速，从而得到的模型比基于CNN/ rnn的方案快一个数量级。此外，我们分析了SASRec的复杂性和可扩展性，开展了全面的消融研究来展示关键部件的影响，并可视化了注意力权重来定性地揭示模型工作。
<a name="AqYFQ"></a>
# Related work
有几项工作与我们密切相关。我们首先讨论一般推荐，然后是时间推荐，然后讨论顺序推荐(特别是mc和rnn)。最后，我们介绍了注意机制，特别是自我注意模块，这是我们的模型的核心。
<a name="R421Y"></a>
##  General Recommendation
推荐系统专注于基于历史反馈(如点击、购买、点赞)建立用户和商品之间的兼容性模型。用户反馈可以是明确的(例如:评级)或隐含(如点击，购买，评论)[4]，[5]。由于对“未观察到的”(如非购买的)数据的解释的模糊性，为隐含反馈建模可能具有挑战性。为了解决这一问题，提出了逐点[4]和成对[5]方法来解决这一挑战。<br />矩阵分解(MF)方法试图揭示潜在的维度来表示用户的偏好和商品的属性，并通过用户和商品嵌入[6]、[7]之间的内积来估计交互作用。此外，另一种工作是基于项目相似模型(ISM)，并没有显式地用潜在因素(例如FISM[8])建模每个用户。他们会学习商品之间的相似性矩阵，并通过测量用户与之前互动过的商品的相似性来估计用户对该商品的偏好。<br />最近，由于在相关问题上的成功，各种深度学习技术被引入推荐[9]。其中一项工作旨在使用神经网络提取条目特征(例如图像[10]、[11]、文本[12]、[13]等)，以进行内容感知推荐。另一个方向是寻求取代传统的MF。例如，NeuMF[14]通过多层感知(MLP)估计用户偏好，AutoRec[15]使用自动编码器预测评级。
<a name="JrqFn"></a>
##  Temporal Recommendation
回溯到Netflix奖，时间推荐通过明确地对用户活动的时间戳进行建模，在各种任务上都表现出了强大的性能。TimeSVD++[16]通过将时间划分为几个部分，并分别对每个部分的用户和项目进行建模，获得了强大的效果。这些模型对于理解表现出显著(短期或长期)时间趋势的数据集至关重要。“过去10年，人们对电影的偏好发生了怎样的变化”，或者“用户在下午4点会访问什么样的企业?””等。)[16]-[18]。序列推荐(或下一项推荐)与此设置略有不同，因为它只考虑操作的顺序，并对独立于时间的顺序模式建模。从本质上讲，序列模型试图基于用户最近的活动来模拟用户行为的“情境”，而不是考虑时间模式本身。
<a name="Xe5nY"></a>
##  Sequential Recommendation
许多顺序推荐系统试图对项目-项目转换矩阵建模，作为在连续项目之间捕获顺序模式的一种方法。例如，FPMC融合了MF项和项-项过渡项，分别捕获长期偏好和短期过渡[1]。从本质上说，所捕获的跃迁是一个一阶马尔科夫链(MC)，而高阶马尔科夫链假设下一个动作与前几个动作有关。	由于最近访问的条目通常是影响用户下一步操作的关键因素(本质上是提供“上下文”)，一阶MC方法表现出强大的性能，特别是在稀疏数据集[19]上。也有采用高阶mc的方法，考虑较多前面项[20]，[21]。其中，卷积序列嵌入(Convolutional Sequence Embedding, Caser)是一种基于cnn的方法，它将前面L项的嵌入矩阵看作是一个“图像”，通过卷积运算提取转换（卷积提取这种隐含的变化特征）[22]。<br />除了基于mc的方法外，另一种方法是利用rnn对用户序列[2]、[23]-[25]进行建模。例如，GRU4Rec使用 Gated Recurrent Units (GRU)对基于会话的推荐[2]进行点击序列建模，改进的版本进一步提高了Top-N推荐性能[26]。在每个时间步骤中，rnn以上一步的状态和当前动作作为输入。这些依赖会降低rnn的效率，尽管像“会话并行”这样的技术已经被提出来提高效率[2]。
<a name="xFRfu"></a>
##  Attention Mechanisms
注意力机制已经被证明在各种任务中都是有效的，比如图片字幕[27]和机器翻译[28]等等。这种机制背后的基本思想是，例如，顺序输出每个项都依赖于模型应该依次关注的某些输入的“相关”部分。另一个好处是，基于注意力的方法通常更容易解释。最近，注意力机制被纳入推荐系统[29]-[31]。例如，注意因子分解机器（注意力因子分解器 Attentional Factorization Machines (AFM)[https://blog.csdn.net/Karty9/article/details/124590975](https://blog.csdn.net/Karty9/article/details/124590975)；[https://blog.csdn.net/Karty9/article/details/118253736?spm=1001.2014.3001.5502](https://blog.csdn.net/Karty9/article/details/118253736?spm=1001.2014.3001.5502)）[30]学习每个特征交互对于内容感知推荐的重要性。<br />然而，上面使用的注意技术本质上是原始模型的一个附加组件(例如，注意力+ RNNs，注意力+FMs等)。最近，一种纯粹基于注意力的序列对序列的方法，transomer[3]，在机器翻译任务上取得了最先进的性能和效率，而此前大都是基于RNN/ cnn的方法[32]，[33]。Transformer模型高度依赖于提出的“自我注意”模块，以捕获句子中的复杂结构，并检索(源语言中的)相关单词，以生成(目标语言中的)下一个单词。受Transformer的启发，我们寻求建立一种新的基于自我注意方法的序列推荐模型，尽管序列推荐的问题与机器翻译有很大的不同，需要专门设计模型。<br />![符号说明](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657605024035-5d169ebd-f263-4c9a-accb-c715e9cd505d.png#clientId=ubd68cdfe-2e47-4&from=paste&height=332&id=u75ee6c05&originHeight=264&originWidth=438&originalType=binary&ratio=1&rotation=0&showTitle=true&size=29816&status=done&style=none&taskId=u2f0709d6-266b-4af6-b71d-9b2432dbf40&title=%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E&width=551 "符号说明")
<a name="u1lei"></a>
# Methodology
用户活动序列![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657605630628-30799c29-4a85-4f15-aedb-fd19596c0cfb.png#clientId=ubd68cdfe-2e47-4&from=paste&height=22&id=ud46a0cdf&originHeight=22&originWidth=175&originalType=binary&ratio=1&rotation=0&showTitle=false&size=2153&status=done&style=none&taskId=u8ca5fe27-5e7e-4737-abb9-9499d778dd5&title=&width=175)，训练中时间步长为t，即模型根据前t项预测下一项，如图1，即由输入序列![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657605819086-c0b4aaff-8b6b-46ee-aedc-f76319720313.png#clientId=ubd68cdfe-2e47-4&from=paste&height=25&id=u31be0bdb&originHeight=25&originWidth=151&originalType=binary&ratio=1&rotation=0&showTitle=false&size=2127&status=done&style=none&taskId=u2a5a4a06-eaef-4a81-8a9b-6b00e6bf8c4&title=&width=151)得到预测后的“位移”序列![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657605877637-84903ba0-4c12-4418-b0d1-81bcb18df211.png#clientId=ubd68cdfe-2e47-4&from=paste&height=24&id=ueab7c1a9&originHeight=24&originWidth=130&originalType=binary&ratio=1&rotation=0&showTitle=false&size=2034&status=done&style=none&taskId=u4fd1e6c7-a80c-4798-bed1-30dfd050ee5&title=&width=130)
<a name="DeHvK"></a>
##  Embedding Layer
先将训练序列![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657605819086-c0b4aaff-8b6b-46ee-aedc-f76319720313.png#clientId=ubd68cdfe-2e47-4&from=paste&height=25&id=VMaxj&originHeight=25&originWidth=151&originalType=binary&ratio=1&rotation=0&showTitle=false&size=2127&status=done&style=none&taskId=u2a5a4a06-eaef-4a81-8a9b-6b00e6bf8c4&title=&width=151)转换为![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657606115199-34d3cf65-022c-4a99-b76c-9f253a9f755b.png#clientId=ubd68cdfe-2e47-4&from=paste&height=20&id=u38cddbe0&originHeight=20&originWidth=141&originalType=binary&ratio=1&rotation=0&showTitle=false&size=1451&status=done&style=none&taskId=u54e740d7-7de0-42aa-8d74-1324ac2a4d7&title=&width=141)，n为模型最大处理长度，（如果序列长度大于n，我们考虑最近的n个动作。如果序列长度小于n，则重复向左侧添加“填充”项，直到长度达到n）<br />创建项embedding矩阵![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657606272366-fd044584-aa03-4a51-9e2e-e995d3521f93.png#clientId=ubd68cdfe-2e47-4&from=paste&height=23&id=ua7ea23e4&originHeight=23&originWidth=88&originalType=binary&ratio=1&rotation=0&showTitle=false&size=1477&status=done&style=none&taskId=uef7ae62d-6de9-47fb-bb7c-1e483080f4c&title=&width=88)，d为潜在维度，获得输入embedding矩阵![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657606593890-14700b4e-c769-4bbf-924e-f446d228c970.png#clientId=ubd68cdfe-2e47-4&from=paste&height=22&id=u794f406b&originHeight=22&originWidth=72&originalType=binary&ratio=1&rotation=0&showTitle=false&size=1308&status=done&style=none&taskId=u5594d445-af1d-48a1-b96c-ceddb14d338&title=&width=72)其中![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657606632033-9f27e2fd-a0f9-4b9c-9348-7dc89b7ad55a.png#clientId=ubd68cdfe-2e47-4&from=paste&height=21&id=u125a211e&originHeight=21&originWidth=74&originalType=binary&ratio=1&rotation=0&showTitle=false&size=1116&status=done&style=none&taskId=ua03c4d1a-8716-4ed9-8764-0bd21b5fc6d&title=&width=74)（零向量用作embedding填充）
<a name="jH6lJ"></a>
### Positional Embedding
由于自注意力模型与RNN和CNN的区别，它不知道前后项关系，因此需要输入一个位置embedding![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657607041589-ccba8439-fd71-49ca-b6fd-bb8d62f89883.png#clientId=ubd68cdfe-2e47-4&from=paste&height=22&id=u49d86f96&originHeight=22&originWidth=78&originalType=binary&ratio=1&rotation=0&showTitle=false&size=1273&status=done&style=none&taskId=uf31d7cb8-f161-4ee7-86ea-4d00bf27919&title=&width=78)到输入embedding中，即<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657607075730-490d1d30-9734-4eaa-8332-e00392e38bf1.png#clientId=ubd68cdfe-2e47-4&from=paste&height=100&id=u7c52ca64&originHeight=100&originWidth=168&originalType=binary&ratio=1&rotation=0&showTitle=false&size=3300&status=done&style=none&taskId=u36b43ca6-d4de-4821-98e7-d7dc1f4c902&title=&width=168)<br />_（[3]中使用固定位置嵌入，但发现在本例中这会导致性能下降。本文在实验中定量和定性地分析了位置嵌入的效果）_
<a name="wfqro"></a>
##  Self-Attention Block
缩放后的点积注意力[3]定义为:![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1657607529092-7afe5959-9a2d-4b82-afe1-5a609a096e3d.png#clientId=ubd68cdfe-2e47-4&from=paste&height=50&id=u4846a18c&originHeight=50&originWidth=372&originalType=binary&ratio=1&rotation=0&showTitle=false&size=6113&status=done&style=none&taskId=ue3593bbe-51f0-4b17-a833-4d6fa7a24af&title=&width=372)<br />其中Q表示查询，K表示键，V表示值(每一行表示一个项)。[https://blog.csdn.net/gary101818/article/details/122807315](https://blog.csdn.net/gary101818/article/details/122807315)<br />[https://zhuanlan.zhihu.com/p/145885498](https://zhuanlan.zhihu.com/p/145885498)<br />[https://blog.csdn.net/sinat_37574187/article/details/119036075](https://blog.csdn.net/sinat_37574187/article/details/119036075)（注意力机制：点积、通用、缩放点积、拼接、相加）注意力层计算所有值的加权和,查询之间的重量和价值j与查询i和关键j之间交互有关（查询i和值j之间的权重和查询i和值j之间的相互作用有关），（比例因子√d是为了避免过于大的内积值,特别是当维数很高）
<a name="QSbaU"></a>
### Self-Attention layer
在机器翻译等 NLP 任务中，注意力机制通常与 K = V 一起使用（例如，使用 RNN 编码器-解码器进行翻译：编码器的隐藏状态是键和值，解码器的隐藏状态是查询）[28]。最近，提出了一种自我注意方法，它使用与查询、键和值相同的对象 [3]。在我们的例子中，self-attention 操作将嵌入E^作为输入，通过线性投影将其转换为三个矩阵，并将它们输入到注意力层：<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1658670313008-8d6f4922-7772-4193-9771-e7249e5504ad.png#clientId=u0a8d50bf-4ec3-4&from=paste&height=45&id=ufc4983f8&originHeight=45&originWidth=488&originalType=binary&ratio=1&rotation=0&showTitle=false&size=6242&status=done&style=none&taskId=u4e14a54a-4a7e-438c-979c-f70c1daa95d&title=&width=488)<br />其中投影矩阵 WQ, WK , WV ∈ Rd×d。投影使模型更加灵活。例如，模型可以学习非对称交互（即<query i, key j> 和 <query j, key i> 可以有不同的交互）
<a name="EFOXv"></a>
### Causality
由于序列的性质，模型在预测 (t + 1)-st 项时应该只考虑前 t 项。然而，自注意力层 (St) 的第 t 个输出包含后续项目的嵌入，这使得模型不适定。因此，我们通过禁止 Qi 和 Kj (j > i) 之间的所有链接来修改注意力。[即显式地将注意力限制为只注意前面的输入的嵌入]。
<a name="ODh53"></a>
### Point-Wise Feed-Forward Network: 
尽管 selfattention 能够使用自适应权重聚合所有先前项目的嵌入，但最终它仍然是一个线性模型。为了赋予模型非线性并考虑不同潜在维度之间的相互作用，我们将逐点前馈网络相同地应用于所有 Si（共享参数）：<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/29360045/1658671625141-428bf2f8-7509-4cdd-aa50-b39049306e04.png#clientId=u0a8d50bf-4ec3-4&from=paste&height=33&id=u1ef00ae1&originHeight=33&originWidth=513&originalType=binary&ratio=1&rotation=0&showTitle=false&size=7009&status=done&style=none&taskId=u79350b3d-7cf3-4ce1-8b96-c60344bf33a&title=&width=513)<br />（FFN：前馈神经网络，基本上可看做多层感知机MLP；ReLU：** 线性整流函数**（Rectified Linear Unit, **ReLU**）,又称**修正线性单元**, 是一种人工神经网络中常用的激活函数（activation function），通常指代以斜坡函数及其变种为代表的非线性函数。比较常用的线性整流函数有斜坡函数）<br /> 
<a name="SUQK7"></a>
##  Stacking Self-Attention Blocks
在第一个 self-attention 块之后，Fi 本质上聚合了所有先前项目的嵌入（即 ̂ Ej ，j ≤ i）。然而，通过另一个基于 F 的自注意力块来学习更复杂的项目转换可能是有用的。具体来说，我们堆叠自注意力块（即，一个自注意力层和一个前馈网络）和第 b 个(b > 1) 块定义为：

 

 <br /> 











